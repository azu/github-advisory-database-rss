<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://azu.github.io/github-advisory-database-rss/rust.rss</id>
    <title>Security Advisory for Rust crates</title>
    <updated>2023-03-24T19:01:22.359Z</updated>
    <generator>github-advisory-database-rss</generator>
    <link rel="alternate" href="https://github.com/advisories?query=type%3Areviewed+ecosystem%3Arust"/>
    <subtitle>Security Advisory for Rust crates on GitHub</subtitle>
    <rights>github-advisory-database-rss</rights>
    <category term="CRITICAL"/>
    <category term="HIGH"/>
    <category term="MODERATE"/>
    <category term="LOW"/>
    <entry>
        <title type="html"><![CDATA[[deno_runtime] Deno improperly handles resizable ArrayBuffer]]></title>
        <id>https://github.com/advisories/GHSA-c25x-cm9x-qqgx</id>
        <link href="https://github.com/advisories/GHSA-c25x-cm9x-qqgx"/>
        <updated>2023-03-24T13:32:17.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p><a href="https://github.com/tc39/proposal-resizablearraybuffer">Resizable ArrayBuffers</a> passed to asynchronous native functions that are shrunk during the asynchronous operation could result in an out-of-bound read/write.</p>
<p>It is unlikely that this has been exploited in the wild, as the only version affected is Deno 1.32.0.</p>
<p>Deno Deploy users are not affected.</p>
<h3 id="patches">Patches</h3>
<p>The problem has been resolved by disabling resizable ArrayBuffers temporarily in Deno 1.32.1. Deno 1.32.2 will re-enable resizable ArrayBuffers with a proper fix.</p>
<h3 id="workarounds">Workarounds</h3>
<p>Upgrade to Deno 1.32.1, or run with <code>--v8-flags=--no-harmony-rab-gsab</code> to disable resizable ArrayBuffers.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/denoland/deno/security/advisories/GHSA-c25x-cm9x-qqgx">https://github.com/denoland/deno/security/advisories/GHSA-c25x-cm9x-qqgx</a></li>
<li><a href="https://github.com/denoland/deno/pull/18395">https://github.com/denoland/deno/pull/18395</a></li>
<li><a href="https://github.com/denoland/deno/releases/tag/v1.32.1">https://github.com/denoland/deno/releases/tag/v1.32.1</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-28445">https://nvd.nist.gov/vuln/detail/CVE-2023-28445</a></li>
<li><a href="https://github.com/advisories/GHSA-c25x-cm9x-qqgx">https://github.com/advisories/GHSA-c25x-cm9x-qqgx</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-c25x-cm9x-qqgx</uri>
        </author>
        <category label="severity" term="CRITICAL"/>
        <published>2023-03-23T23:13:25.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[serde_v8] Deno improperly handles resizable ArrayBuffer]]></title>
        <id>https://github.com/advisories/GHSA-c25x-cm9x-qqgx</id>
        <link href="https://github.com/advisories/GHSA-c25x-cm9x-qqgx"/>
        <updated>2023-03-24T13:32:17.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p><a href="https://github.com/tc39/proposal-resizablearraybuffer">Resizable ArrayBuffers</a> passed to asynchronous native functions that are shrunk during the asynchronous operation could result in an out-of-bound read/write.</p>
<p>It is unlikely that this has been exploited in the wild, as the only version affected is Deno 1.32.0.</p>
<p>Deno Deploy users are not affected.</p>
<h3 id="patches">Patches</h3>
<p>The problem has been resolved by disabling resizable ArrayBuffers temporarily in Deno 1.32.1. Deno 1.32.2 will re-enable resizable ArrayBuffers with a proper fix.</p>
<h3 id="workarounds">Workarounds</h3>
<p>Upgrade to Deno 1.32.1, or run with <code>--v8-flags=--no-harmony-rab-gsab</code> to disable resizable ArrayBuffers.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/denoland/deno/security/advisories/GHSA-c25x-cm9x-qqgx">https://github.com/denoland/deno/security/advisories/GHSA-c25x-cm9x-qqgx</a></li>
<li><a href="https://github.com/denoland/deno/pull/18395">https://github.com/denoland/deno/pull/18395</a></li>
<li><a href="https://github.com/denoland/deno/releases/tag/v1.32.1">https://github.com/denoland/deno/releases/tag/v1.32.1</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-28445">https://nvd.nist.gov/vuln/detail/CVE-2023-28445</a></li>
<li><a href="https://github.com/advisories/GHSA-c25x-cm9x-qqgx">https://github.com/advisories/GHSA-c25x-cm9x-qqgx</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-c25x-cm9x-qqgx</uri>
        </author>
        <category label="severity" term="CRITICAL"/>
        <published>2023-03-23T23:13:25.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[Deno] Deno improperly handles resizable ArrayBuffer]]></title>
        <id>https://github.com/advisories/GHSA-c25x-cm9x-qqgx</id>
        <link href="https://github.com/advisories/GHSA-c25x-cm9x-qqgx"/>
        <updated>2023-03-24T13:32:17.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p><a href="https://github.com/tc39/proposal-resizablearraybuffer">Resizable ArrayBuffers</a> passed to asynchronous native functions that are shrunk during the asynchronous operation could result in an out-of-bound read/write.</p>
<p>It is unlikely that this has been exploited in the wild, as the only version affected is Deno 1.32.0.</p>
<p>Deno Deploy users are not affected.</p>
<h3 id="patches">Patches</h3>
<p>The problem has been resolved by disabling resizable ArrayBuffers temporarily in Deno 1.32.1. Deno 1.32.2 will re-enable resizable ArrayBuffers with a proper fix.</p>
<h3 id="workarounds">Workarounds</h3>
<p>Upgrade to Deno 1.32.1, or run with <code>--v8-flags=--no-harmony-rab-gsab</code> to disable resizable ArrayBuffers.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/denoland/deno/security/advisories/GHSA-c25x-cm9x-qqgx">https://github.com/denoland/deno/security/advisories/GHSA-c25x-cm9x-qqgx</a></li>
<li><a href="https://github.com/denoland/deno/pull/18395">https://github.com/denoland/deno/pull/18395</a></li>
<li><a href="https://github.com/denoland/deno/releases/tag/v1.32.1">https://github.com/denoland/deno/releases/tag/v1.32.1</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-28445">https://nvd.nist.gov/vuln/detail/CVE-2023-28445</a></li>
<li><a href="https://github.com/advisories/GHSA-c25x-cm9x-qqgx">https://github.com/advisories/GHSA-c25x-cm9x-qqgx</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-c25x-cm9x-qqgx</uri>
        </author>
        <category label="severity" term="CRITICAL"/>
        <published>2023-03-23T23:13:25.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[time] Segmentation fault in time]]></title>
        <id>https://github.com/advisories/GHSA-wcg3-cvx6-7396</id>
        <link href="https://github.com/advisories/GHSA-wcg3-cvx6-7396"/>
        <updated>2023-03-23T23:05:19.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>Unix-like operating systems may segfault due to dereferencing a dangling pointer in specific circumstances. This requires an environment variable to be set in a different thread than the affected functions. This may occur without the user's knowledge, notably in a third-party library.</p>
<p>The affected functions from time 0.2.7 through 0.2.22 are:</p>
<ul>
<li><code>time::UtcOffset::local_offset_at</code></li>
<li><code>time::UtcOffset::try_local_offset_at</code></li>
<li><code>time::UtcOffset::current_local_offset</code></li>
<li><code>time::UtcOffset::try_current_local_offset</code></li>
<li><code>time::OffsetDateTime::now_local</code></li>
<li><code>time::OffsetDateTime::try_now_local</code></li>
</ul>
<p>The affected functions in time 0.1 (all versions) are:</p>
<ul>
<li><code>at</code></li>
<li><code>at_utc</code></li>
<li><code>now</code></li>
</ul>
<p>Non-Unix targets (including Windows and wasm) are unaffected.</p>
<h3 id="patches">Patches</h3>
<p>In some versions of <code>time</code>, the internal method that determines the local offset has been modified to always return <code>None</code> on the affected operating systems. This has the effect of returning an <code>Err</code> on the <code>try_*</code> methods and <code>UTC</code> on the non-<code>try_*</code> methods. In later versions, <code>time</code> will attempt to determine the number of threads running in the process. If the process is single-threaded, the call will proceed as its safety invariant is upheld.</p>
<p>Users and library authors with time in their dependency tree must perform <code>cargo update</code>, which will pull in the updated, unaffected code.</p>
<p>Users of time 0.1 do not have a patch and must upgrade to an unaffected version: time 0.2.23 or greater or the 0.3 series.</p>
<h3 id="workarounds">Workarounds</h3>
<p>Library authors must ensure that the program only has one running thread at the time of calling any affected method. Binary authors may do the same and/or ensure that no other thread is actively mutating the environment.</p>
<h3 id="references">References</h3>
<p><a href="https://github.com/time-rs/time/issues/293">time-rs/time#293</a>.</p>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/time-rs/time/security/advisories/GHSA-wcg3-cvx6-7396">https://github.com/time-rs/time/security/advisories/GHSA-wcg3-cvx6-7396</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2020-26235">https://nvd.nist.gov/vuln/detail/CVE-2020-26235</a></li>
<li><a href="https://github.com/time-rs/time/issues/293">https://github.com/time-rs/time/issues/293</a></li>
<li><a href="https://rustsec.org/advisories/RUSTSEC-2020-0071.html">https://rustsec.org/advisories/RUSTSEC-2020-0071.html</a></li>
<li><a href="https://crates.io/crates/time/0.2.23">https://crates.io/crates/time/0.2.23</a></li>
<li><a href="https://github.com/advisories/GHSA-wcg3-cvx6-7396">https://github.com/advisories/GHSA-wcg3-cvx6-7396</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-wcg3-cvx6-7396</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2021-08-25T20:56:46.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[rmp-serde] `rmp-serde` `Raw` and `RawRef` may crash when receiving invalid UTF-8]]></title>
        <id>https://github.com/advisories/GHSA-255r-3prx-mf99</id>
        <link href="https://github.com/advisories/GHSA-255r-3prx-mf99"/>
        <updated>2023-03-22T22:21:50.000Z</updated>
        <content type="html"><![CDATA[<p>It was found that <code>Raw::from_utf8</code> expects valid UTF-8. If invalid UTF-8 is received it can cause the process to crash.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/3Hren/msgpack-rust/issues/305">https://github.com/3Hren/msgpack-rust/issues/305</a></li>
<li><a href="https://rustsec.org/advisories/RUSTSEC-2022-0092.html">https://rustsec.org/advisories/RUSTSEC-2022-0092.html</a></li>
<li><a href="https://github.com/advisories/GHSA-255r-3prx-mf99">https://github.com/advisories/GHSA-255r-3prx-mf99</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-255r-3prx-mf99</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2023-03-22T22:21:49.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[frontier] Frontier's modexp precompile is slow for even modulus]]></title>
        <id>https://github.com/advisories/GHSA-fcmm-54jp-7vf6</id>
        <link href="https://github.com/advisories/GHSA-fcmm-54jp-7vf6"/>
        <updated>2023-03-22T21:40:47.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>Frontier's <code>modexp</code> precompile uses <code>num-bigint</code> crate under the hood. <a href="https://github.com/rust-num/num-bigint/blob/6f2b8e0fc218dbd0f49bebb8db2d1a771fe6bafa/src/biguint/power.rs#L134">In the implementation</a>, the cases for modulus being even and modulus being odd are treated separately. Odd modulus uses the fast Montgomery multiplication, and even modulus uses the slow plain power algorithm. This gas cost discrepancy was not accounted for in the <code>modexp</code> precompile, leading to possible denial of service attacks.</p>
<h3 id="patches">Patches</h3>
<p>No fixes for <code>num-bigint</code> is currently available, and thus this advisory will be first fixed in the short term by raising the gas costs for even modulus, and in the long term fixing it in <code>num-bigint</code> or switching to another modexp implementation.</p>
<p>The short-term fix for Frontier is deployed at <a href="https://github.com/paritytech/frontier/pull/1017">PR 1017</a>.</p>
<p>The recommendations are as follows:</p>
<ul>
<li>If you anticipate malicious validators, it's recommended to issue an emergency runtime upgrade as soon as possible.</li>
<li>If you do not anticipate malicious validators, it's recommended to issue a normal runtime upgrade, as Substrate has builtin timeout protection when validators are building blocks.</li>
</ul>
<h3 id="workarounds">Workarounds</h3>
<p>None.</p>
<h3 id="references">References</h3>
<p>A similar issue was presented in Geth's implementation and the fix can be found <a href="https://go-review.googlesource.com/c/go/+/420897">here</a>.</p>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/paritytech/frontier/security/advisories/GHSA-fcmm-54jp-7vf6">https://github.com/paritytech/frontier/security/advisories/GHSA-fcmm-54jp-7vf6</a></li>
<li><a href="https://github.com/paritytech/frontier/pull/1017">https://github.com/paritytech/frontier/pull/1017</a></li>
<li><a href="https://github.com/paritytech/frontier/commit/5af12e94d7dfc8a0208a290643a800f55de7b219">https://github.com/paritytech/frontier/commit/5af12e94d7dfc8a0208a290643a800f55de7b219</a></li>
<li><a href="https://github.com/rust-num/num-bigint/blob/6f2b8e0fc218dbd0f49bebb8db2d1a771fe6bafa/src/biguint/power.rs#L134">https://github.com/rust-num/num-bigint/blob/6f2b8e0fc218dbd0f49bebb8db2d1a771fe6bafa/src/biguint/power.rs#L134</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-28431">https://nvd.nist.gov/vuln/detail/CVE-2023-28431</a></li>
<li><a href="https://github.com/advisories/GHSA-fcmm-54jp-7vf6">https://github.com/advisories/GHSA-fcmm-54jp-7vf6</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-fcmm-54jp-7vf6</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2023-03-21T22:31:30.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[stb_image] NULL pointer derefernce in `stb_image`]]></title>
        <id>https://github.com/advisories/GHSA-ppjr-267j-5p9x</id>
        <link href="https://github.com/advisories/GHSA-ppjr-267j-5p9x"/>
        <updated>2023-03-20T21:12:09.000Z</updated>
        <content type="html"><![CDATA[<p>A bug in error handling in the <code>stb_image</code> C library could cause a NULL pointer dereference when attempting to load an invalid or unsupported image file.  This is fixed in version 0.2.5 and later of the <code>stb_image</code> Rust crate, by patching the C code to correctly handle NULL pointers.</p>
<p>Thank you to GitHub user 0xdd96 for finding and fixing this vulnerability.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/servo/rust-stb-image/pull/102">https://github.com/servo/rust-stb-image/pull/102</a></li>
<li><a href="https://rustsec.org/advisories/RUSTSEC-2023-0021.html">https://rustsec.org/advisories/RUSTSEC-2023-0021.html</a></li>
<li><a href="https://github.com/advisories/GHSA-ppjr-267j-5p9x">https://github.com/advisories/GHSA-ppjr-267j-5p9x</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-ppjr-267j-5p9x</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2023-03-20T21:11:58.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[russh] russh may use insecure Diffie-Hellman keys]]></title>
        <id>https://github.com/advisories/GHSA-cqvm-j2r2-hwpg</id>
        <link href="https://github.com/advisories/GHSA-cqvm-j2r2-hwpg"/>
        <updated>2023-03-23T22:30:22.000Z</updated>
        <content type="html"><![CDATA[<h3 id="summary">Summary</h3>
<p>Diffie-Hellman key validation is insufficient, which can lead to insecure shared secrets and therefore breaks confidentiality.</p>
<h3 id="details">Details</h3>
<p>Russh does not validate Diffie-Hellman keys.</p>
<p>It accepts received DH public keys $e$ where $e&lt;0$, $e=1$, or $e \geq p-1$ from a misbehaving peer annd successfully performs key exchange.</p>
<p>This is a violation of <a href="https://www.rfc-editor.org/rfc/rfc4253#section-8">RFC 4253, section 8</a> and <a href="https://www.rfc-editor.org/rfc/rfc8268#section-4">RFC 8268, section 4</a>, which state that:</p>
<blockquote>
<p>DH Public Key values MUST be checked and both conditions:</p>
<ul>
<li>$1 &lt; e &lt; p-1$</li>
<li>$1 &lt; f &lt; p-1$</li>
</ul>
<p>MUST be true.  Values not within these bounds MUST NOT be sent or
accepted by either side.  If either one of these conditions is
violated, then the key exchange fails.</p>
</blockquote>
<p>For example, a DH client public key $e=1$ would mean that the shared secret that the server calculates is always $K = e^y \mod{p} = 1^y \mod{p} = 1$.
In other cases, an insecure order-2 subgroup may be used.</p>
<p>Also, the code does not look like it ensures that the generated secret key $y$ is in the valid interval $0 &lt; y &lt; q$ (or, if russh is the client, that the secret key $x$ satisfies $1 &lt; x &lt; q$):
<a href="https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L72-L76">https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L72-L76</a>
For example, <code>rng.gen_biguint()</code> might return a number consisting of zeroes, so that $y = 0$.</p>
<p>The public key is not validated either:
<a href="https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L78-L81">https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L78-L81</a></p>
<h3 id="impact">Impact</h3>
<p>Due to the issues in the DH key generation, I think any connection that uses Diffie-Hellman key exchange is affected.
Connections between a russh client and server or those of a russh peer with some other misbehaving peer are most likely to be problematic. These may vulnerable to eavesdropping.</p>
<p>Most other implementations reject such keys, so this is mainly an interoperability issue in such a case.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/warp-tech/russh/security/advisories/GHSA-cqvm-j2r2-hwpg">https://github.com/warp-tech/russh/security/advisories/GHSA-cqvm-j2r2-hwpg</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-28113">https://nvd.nist.gov/vuln/detail/CVE-2023-28113</a></li>
<li><a href="https://github.com/warp-tech/russh/commit/45d2d82930bf4a675bd57abfafec8fe4065befcd">https://github.com/warp-tech/russh/commit/45d2d82930bf4a675bd57abfafec8fe4065befcd</a></li>
<li><a href="https://github.com/warp-tech/russh/commit/d831a3716d3719dc76f091fcea9d94bd4ef97c6e">https://github.com/warp-tech/russh/commit/d831a3716d3719dc76f091fcea9d94bd4ef97c6e</a></li>
<li><a href="https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L72-L76">https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L72-L76</a></li>
<li><a href="https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L78-L81">https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L78-L81</a></li>
<li><a href="https://github.com/warp-tech/russh/releases/tag/v0.36.2">https://github.com/warp-tech/russh/releases/tag/v0.36.2</a></li>
<li><a href="https://github.com/warp-tech/russh/releases/tag/v0.37.1">https://github.com/warp-tech/russh/releases/tag/v0.37.1</a></li>
<li><a href="https://github.com/advisories/GHSA-cqvm-j2r2-hwpg">https://github.com/advisories/GHSA-cqvm-j2r2-hwpg</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-cqvm-j2r2-hwpg</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2023-03-17T14:42:45.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[russh] russh may use insecure Diffie-Hellman keys]]></title>
        <id>https://github.com/advisories/GHSA-cqvm-j2r2-hwpg</id>
        <link href="https://github.com/advisories/GHSA-cqvm-j2r2-hwpg"/>
        <updated>2023-03-23T22:30:22.000Z</updated>
        <content type="html"><![CDATA[<h3 id="summary">Summary</h3>
<p>Diffie-Hellman key validation is insufficient, which can lead to insecure shared secrets and therefore breaks confidentiality.</p>
<h3 id="details">Details</h3>
<p>Russh does not validate Diffie-Hellman keys.</p>
<p>It accepts received DH public keys $e$ where $e&lt;0$, $e=1$, or $e \geq p-1$ from a misbehaving peer annd successfully performs key exchange.</p>
<p>This is a violation of <a href="https://www.rfc-editor.org/rfc/rfc4253#section-8">RFC 4253, section 8</a> and <a href="https://www.rfc-editor.org/rfc/rfc8268#section-4">RFC 8268, section 4</a>, which state that:</p>
<blockquote>
<p>DH Public Key values MUST be checked and both conditions:</p>
<ul>
<li>$1 &lt; e &lt; p-1$</li>
<li>$1 &lt; f &lt; p-1$</li>
</ul>
<p>MUST be true.  Values not within these bounds MUST NOT be sent or
accepted by either side.  If either one of these conditions is
violated, then the key exchange fails.</p>
</blockquote>
<p>For example, a DH client public key $e=1$ would mean that the shared secret that the server calculates is always $K = e^y \mod{p} = 1^y \mod{p} = 1$.
In other cases, an insecure order-2 subgroup may be used.</p>
<p>Also, the code does not look like it ensures that the generated secret key $y$ is in the valid interval $0 &lt; y &lt; q$ (or, if russh is the client, that the secret key $x$ satisfies $1 &lt; x &lt; q$):
<a href="https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L72-L76">https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L72-L76</a>
For example, <code>rng.gen_biguint()</code> might return a number consisting of zeroes, so that $y = 0$.</p>
<p>The public key is not validated either:
<a href="https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L78-L81">https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L78-L81</a></p>
<h3 id="impact">Impact</h3>
<p>Due to the issues in the DH key generation, I think any connection that uses Diffie-Hellman key exchange is affected.
Connections between a russh client and server or those of a russh peer with some other misbehaving peer are most likely to be problematic. These may vulnerable to eavesdropping.</p>
<p>Most other implementations reject such keys, so this is mainly an interoperability issue in such a case.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/warp-tech/russh/security/advisories/GHSA-cqvm-j2r2-hwpg">https://github.com/warp-tech/russh/security/advisories/GHSA-cqvm-j2r2-hwpg</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-28113">https://nvd.nist.gov/vuln/detail/CVE-2023-28113</a></li>
<li><a href="https://github.com/warp-tech/russh/commit/45d2d82930bf4a675bd57abfafec8fe4065befcd">https://github.com/warp-tech/russh/commit/45d2d82930bf4a675bd57abfafec8fe4065befcd</a></li>
<li><a href="https://github.com/warp-tech/russh/commit/d831a3716d3719dc76f091fcea9d94bd4ef97c6e">https://github.com/warp-tech/russh/commit/d831a3716d3719dc76f091fcea9d94bd4ef97c6e</a></li>
<li><a href="https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L72-L76">https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L72-L76</a></li>
<li><a href="https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L78-L81">https://github.com/warp-tech/russh/blob/master/russh/src/kex/dh/groups.rs#L78-L81</a></li>
<li><a href="https://github.com/warp-tech/russh/releases/tag/v0.36.2">https://github.com/warp-tech/russh/releases/tag/v0.36.2</a></li>
<li><a href="https://github.com/warp-tech/russh/releases/tag/v0.37.1">https://github.com/warp-tech/russh/releases/tag/v0.37.1</a></li>
<li><a href="https://github.com/advisories/GHSA-cqvm-j2r2-hwpg">https://github.com/advisories/GHSA-cqvm-j2r2-hwpg</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-cqvm-j2r2-hwpg</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2023-03-17T14:42:45.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[raw-cpuid] Deserialization of Untrusted Data in rust-cpuid]]></title>
        <id>https://github.com/advisories/GHSA-w428-f65r-h4q2</id>
        <link href="https://github.com/advisories/GHSA-w428-f65r-h4q2"/>
        <updated>2023-03-16T17:39:47.000Z</updated>
        <content type="html"><![CDATA[<p>An issue was discovered in the raw-cpuid crate before 9.1.1 for Rust. If the serialize feature is used (which is not the the default), a Deserialize operation may lack sufficient validation, leading to memory corruption or a panic.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2021-45687">https://nvd.nist.gov/vuln/detail/CVE-2021-45687</a></li>
<li><a href="https://raw.githubusercontent.com/rustsec/advisory-db/main/crates/raw-cpuid/RUSTSEC-2021-0089.md">https://raw.githubusercontent.com/rustsec/advisory-db/main/crates/raw-cpuid/RUSTSEC-2021-0089.md</a></li>
<li><a href="https://rustsec.org/advisories/RUSTSEC-2021-0089.html">https://rustsec.org/advisories/RUSTSEC-2021-0089.html</a></li>
<li><a href="https://github.com/advisories/GHSA-w428-f65r-h4q2">https://github.com/advisories/GHSA-w428-f65r-h4q2</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-w428-f65r-h4q2</uri>
        </author>
        <category label="severity" term="CRITICAL"/>
        <published>2022-01-06T22:13:12.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[mongodb] Exposure of Sensitive Information to an Unauthorized Actor in MongoDB Rust Driver]]></title>
        <id>https://github.com/advisories/GHSA-4rjr-3gj2-5crq</id>
        <link href="https://github.com/advisories/GHSA-4rjr-3gj2-5crq"/>
        <updated>2023-03-16T17:37:54.000Z</updated>
        <content type="html"><![CDATA[<p>Specific MongoDB Rust Driver versions can include credentials used by the connection pool to authenticate connections in the monitoring event that is emitted when the pool is created. The user's logging infrastructure could then potentially ingest these events and unexpectedly leak the credentials. Note that such monitoring is not enabled by default.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2021-20332">https://nvd.nist.gov/vuln/detail/CVE-2021-20332</a></li>
<li><a href="https://jira.mongodb.org/browse/RUST-591">https://jira.mongodb.org/browse/RUST-591</a></li>
<li><a href="https://github.com/mongodb/mongo-rust-driver/commit/9e8782b1bb1104e5399c073b553719c262d4463c">https://github.com/mongodb/mongo-rust-driver/commit/9e8782b1bb1104e5399c073b553719c262d4463c</a></li>
<li><a href="https://github.com/advisories/GHSA-4rjr-3gj2-5crq">https://github.com/advisories/GHSA-4rjr-3gj2-5crq</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-4rjr-3gj2-5crq</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2022-05-24T19:09:36.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[wasmtime] Wasmtime out of bounds read/write with zero-memory-pages configuration]]></title>
        <id>https://github.com/advisories/GHSA-44mr-8vmm-wjhg</id>
        <link href="https://github.com/advisories/GHSA-44mr-8vmm-wjhg"/>
        <updated>2023-03-16T16:18:44.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>There is a bug in Wasmtime's implementation of its pooling instance allocator when the allocator is configured to give WebAssembly instances a maximum of zero pages of memory. In this configuration the virtual memory mapping for WebAssembly memories did not meet the compiler-required configuration requirements for safely executing WebAssembly modules. Wasmtime's default settings require virtual memory page faults to indicate that wasm reads/writes are out-of-bounds, but the pooling allocator's configuration would not create an appropriate virtual memory mapping for this meaning out of bounds reads/writes can successfully read/write memory unrelated to the wasm sandbox within range of the base address of the memory mapping created by the pooling allocator.</p>
<p>This bug can only be triggered by setting <a href="https://docs.rs/wasmtime/2.0.1/wasmtime/struct.InstanceLimits.html#structfield.memory_pages"><code>InstanceLimits::memory_pages</code></a> to zero. This is expected to be a very rare configuration since this means that wasm modules cannot allocate any pages of linear memory. All wasm modules produced by all current toolchains are highly likely to use linear memory, so it's expected to be unlikely that this configuration is set to zero by any production embedding of Wasmtime, hence the low severity of this bug despite the critical consequences.</p>
<h3 id="patches">Patches</h3>
<p>This bug has been patched and users should upgrade to Wasmtime 2.0.2.</p>
<h3 id="workarounds">Workarounds</h3>
<p>One way to mitigate this issue is to disable usage of the pooling allocator. Note that the pooling allocator is not enabled by default.</p>
<p>This bug can also only be worked around by increasing the <code>memory_pages</code> allotment when configuring the pooling allocator to a value greater than zero. If an embedding wishes to still prevent memory from actually being used then the <code>Store::limiter</code> method can be used to dynamically disallow growth of memory beyond 0 bytes large. Note that the default <code>memory_pages</code> value is greater than zero.</p>
<p>This bug is not applicable with the default settings of the <code>wasmtime</code> crate.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://docs.rs/wasmtime/2.0.1/wasmtime/struct.Config.html#method.allocation_strategy"><code>Config::allocation_strategy</code></a> - configuration required to enable the pooling allocator.</li>
<li><a href="https://docs.rs/wasmtime/2.0.1/wasmtime/struct.InstanceLimits.html#structfield.memory_pages"><code>InstanceLimits::memory_pages</code></a> - configuration field that, when zero, exhibits this bug.</li>
<li><a href="https://docs.rs/wasmtime/2.0.1/wasmtime/struct.Store.html#method.limiter"><code>Store::limiter</code></a> - means of limiting memory without using <code>memory_pages</code></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/c1HBDDJwNPA">Mailing list announcement</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/e60c3742904ccbb3e26da201c9221c38a4981d72">Patch for the <code>release-2.0.0</code> branch</a></li>
</ul>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to us on <a href="https://bytecodealliance.zulipchat.com/#narrow/stream/217126-wasmtime">the Bytecode Alliance Zulip chat</a></li>
<li>Open an issue in <a href="https://github.com/bytecodealliance/wasmtime/">the bytecodealliance/wasmtime repository</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-44mr-8vmm-wjhg">https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-44mr-8vmm-wjhg</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/e60c3742904ccbb3e26da201c9221c38a4981d72">https://github.com/bytecodealliance/wasmtime/commit/e60c3742904ccbb3e26da201c9221c38a4981d72</a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/c1HBDDJwNPA">https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/c1HBDDJwNPA</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2022-39392">https://nvd.nist.gov/vuln/detail/CVE-2022-39392</a></li>
<li><a href="https://rustsec.org/advisories/RUSTSEC-2022-0076.html">https://rustsec.org/advisories/RUSTSEC-2022-0076.html</a></li>
<li><a href="https://github.com/advisories/GHSA-44mr-8vmm-wjhg">https://github.com/advisories/GHSA-44mr-8vmm-wjhg</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-44mr-8vmm-wjhg</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2022-11-10T21:09:02.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[wasmtime] Wasmtime out of bounds read/write with zero-memory-pages configuration]]></title>
        <id>https://github.com/advisories/GHSA-44mr-8vmm-wjhg</id>
        <link href="https://github.com/advisories/GHSA-44mr-8vmm-wjhg"/>
        <updated>2023-03-16T16:18:44.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>There is a bug in Wasmtime's implementation of its pooling instance allocator when the allocator is configured to give WebAssembly instances a maximum of zero pages of memory. In this configuration the virtual memory mapping for WebAssembly memories did not meet the compiler-required configuration requirements for safely executing WebAssembly modules. Wasmtime's default settings require virtual memory page faults to indicate that wasm reads/writes are out-of-bounds, but the pooling allocator's configuration would not create an appropriate virtual memory mapping for this meaning out of bounds reads/writes can successfully read/write memory unrelated to the wasm sandbox within range of the base address of the memory mapping created by the pooling allocator.</p>
<p>This bug can only be triggered by setting <a href="https://docs.rs/wasmtime/2.0.1/wasmtime/struct.InstanceLimits.html#structfield.memory_pages"><code>InstanceLimits::memory_pages</code></a> to zero. This is expected to be a very rare configuration since this means that wasm modules cannot allocate any pages of linear memory. All wasm modules produced by all current toolchains are highly likely to use linear memory, so it's expected to be unlikely that this configuration is set to zero by any production embedding of Wasmtime, hence the low severity of this bug despite the critical consequences.</p>
<h3 id="patches">Patches</h3>
<p>This bug has been patched and users should upgrade to Wasmtime 2.0.2.</p>
<h3 id="workarounds">Workarounds</h3>
<p>One way to mitigate this issue is to disable usage of the pooling allocator. Note that the pooling allocator is not enabled by default.</p>
<p>This bug can also only be worked around by increasing the <code>memory_pages</code> allotment when configuring the pooling allocator to a value greater than zero. If an embedding wishes to still prevent memory from actually being used then the <code>Store::limiter</code> method can be used to dynamically disallow growth of memory beyond 0 bytes large. Note that the default <code>memory_pages</code> value is greater than zero.</p>
<p>This bug is not applicable with the default settings of the <code>wasmtime</code> crate.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://docs.rs/wasmtime/2.0.1/wasmtime/struct.Config.html#method.allocation_strategy"><code>Config::allocation_strategy</code></a> - configuration required to enable the pooling allocator.</li>
<li><a href="https://docs.rs/wasmtime/2.0.1/wasmtime/struct.InstanceLimits.html#structfield.memory_pages"><code>InstanceLimits::memory_pages</code></a> - configuration field that, when zero, exhibits this bug.</li>
<li><a href="https://docs.rs/wasmtime/2.0.1/wasmtime/struct.Store.html#method.limiter"><code>Store::limiter</code></a> - means of limiting memory without using <code>memory_pages</code></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/c1HBDDJwNPA">Mailing list announcement</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/e60c3742904ccbb3e26da201c9221c38a4981d72">Patch for the <code>release-2.0.0</code> branch</a></li>
</ul>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to us on <a href="https://bytecodealliance.zulipchat.com/#narrow/stream/217126-wasmtime">the Bytecode Alliance Zulip chat</a></li>
<li>Open an issue in <a href="https://github.com/bytecodealliance/wasmtime/">the bytecodealliance/wasmtime repository</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-44mr-8vmm-wjhg">https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-44mr-8vmm-wjhg</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/e60c3742904ccbb3e26da201c9221c38a4981d72">https://github.com/bytecodealliance/wasmtime/commit/e60c3742904ccbb3e26da201c9221c38a4981d72</a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/c1HBDDJwNPA">https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/c1HBDDJwNPA</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2022-39392">https://nvd.nist.gov/vuln/detail/CVE-2022-39392</a></li>
<li><a href="https://rustsec.org/advisories/RUSTSEC-2022-0076.html">https://rustsec.org/advisories/RUSTSEC-2022-0076.html</a></li>
<li><a href="https://github.com/advisories/GHSA-44mr-8vmm-wjhg">https://github.com/advisories/GHSA-44mr-8vmm-wjhg</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-44mr-8vmm-wjhg</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2022-11-10T21:09:02.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[out-reference] `out_reference::Out::from_raw` should be `unsafe`]]></title>
        <id>https://github.com/advisories/GHSA-p7mj-xvxg-grff</id>
        <link href="https://github.com/advisories/GHSA-p7mj-xvxg-grff"/>
        <updated>2023-03-13T20:49:28.000Z</updated>
        <content type="html"><![CDATA[<p><code>Out::from_raw</code> in affected versions allows writing a value to invalid memory address without requiring <code>unsafe</code>.</p>
<p>The soundness issue has been addressed by making <code>Out::from_raw</code> an unsafe function.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/RustyYato/out-ref/issues/1">https://github.com/RustyYato/out-ref/issues/1</a></li>
<li><a href="https://rustsec.org/advisories/RUSTSEC-2021-0152.html">https://rustsec.org/advisories/RUSTSEC-2021-0152.html</a></li>
<li><a href="https://github.com/advisories/GHSA-p7mj-xvxg-grff">https://github.com/advisories/GHSA-p7mj-xvxg-grff</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-p7mj-xvxg-grff</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2023-03-13T20:49:24.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[cranelift-codegen] wasmtime vulnerable to guest-controlled out-of-bounds read/write on x86_64]]></title>
        <id>https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</id>
        <link href="https://github.com/advisories/GHSA-ff4p-7xrq-q5r8"/>
        <updated>2023-03-09T00:10:54.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>Wasmtime's code generator, Cranelift, has a bug on x86_64 targets where address-mode computation mistakenly would calculate a 35-bit effective address instead of WebAssembly's defined 33-bit effective address. This bug means that, with default codegen settings, a wasm-controlled load/store operation could read/write addresses up to 35 bits away from the base of linear memory. Wasmtime's default sandbox settings provide up to 6G of protection from the base of linear memory to guarantee that any memory access in that range will be semantically correct. Due to this bug, however, addresses up to <code>0xffffffff * 8 + 0x7ffffffc = 36507222004 = ~34G</code> bytes away from the base of linear memory are possible from guest code. This means that the virtual memory 6G away from the base of linear memory up to ~34G away can be read/written by a malicious module.</p>
<p>This out of bounds read/write is not semantically correct and poses a threat as an arbitrary read/write within ~34G of linear memory away from the base of a wasm module's linear memory. A guest module can, without the knowledge of the embedder, read/write memory in this region. The memory may belong to other WebAssembly instances when using the pooling allocator, for example. The memory may also belong to the embedder, depending on address layout.</p>
<p>Embedders do not have a necessarily reliable means of detecting when this happens. Wasm loads/stores are allowed to cause machine segfaults meaning that an invalid read/write would be translated to a nominal WebAssembly trap. This means that a malicious module in the worst case silently reads/writes memory outside its bounds and in the "best" case looks like a normal "something trapped here" during its execution. This makes it difficult to retroactively determine whether this bug has been exploited on hosts. Affected embedders are recommended to analyze preexisting wasm modules to see if they're affected by the incorrect codegen rules and possibly correlate that with an anomalous number of traps during historical execution to locate possibly suspicious modules.</p>
<p>The specific bug in Cranelift's x86_64 backend is that a WebAssembly address which is left-shifted by a constant amount from 1 to 3 will get folded into x86_64's addressing modes which perform shifts. For example <code>(i32.load (i32.shl (local.get 0) (i32.const 3)))</code> loads from the WebAssembly address <code>$local0 &lt;&lt; 3</code>. When translated to Cranelift the <code>$local0 &lt;&lt; 3</code> computation, a 32-bit value, is zero-extended to a 64-bit value and then added to the base address of linear memory. Cranelift would generate an instruction of the form <code>movl (%base, %local0, 8), %dst</code> which calculates <code>%base + %local0 &lt;&lt; 3</code>. The bug here, however, is that the address computation happens with 64-bit values, where the <code>$local0 &lt;&lt; 3</code> computation was supposed to be truncated to a 32-bit value. This means that <code>%local0</code>, which can use up to 32-bits for an address, gets 3 extra bits of address space to be accessible via this <code>movl</code> instruction.</p>
<p>The fix in Cranelift is to remove the erroneous lowering rules in the backend which handle these zero-extended expressions. The above example is then translated to <code>movl %local0, %temp; shl $3, %temp; movl (%base, %temp), %dst</code> which correctly truncates the intermediate computation of <code>%local0 &lt;&lt; 3</code> to 32-bits inside the <code>%temp</code> register which is then added to the <code>%base</code> value.</p>
<h3 id="patches">Patches</h3>
<p>Wasmtime version 4.0.1, 5.0.1, and 6.0.1 have been released and have all been patched to no longer contain the erroneous lowering rules.</p>
<h3 id="workarounds">Workarounds</h3>
<p>While updating Wasmtime is recommended, there are a number of possible workarounds that embedders can employ to mitigate this issue if updating is not possible. Note that none of these workarounds are on-by-default and require explicit configuration:</p>
<ul>
<li>The <code>Config::static_memory_maximum_size(0)</code> option can be used to force all accesses to linear memory to be explicitly bounds-checked. This will perform a bounds check separately from the address-mode computation which correctly calculates the effective address of a load/store. Note that this can have a large impact on the execution performance of WebAssembly modules.</li>
<li>The <code>Config::static_memory_guard_size(1 &lt;&lt; 36)</code> option can be used to greatly increase the guard pages placed after linear memory. This will guarantee that memory accesses up-to-34G away are guaranteed to be semantically correct by reserving unmapped memory for the instance. Note that this reserves a very large amount of virtual memory per-instances and can greatly reduce the maximum number of concurrent instances being run.</li>
<li>If using a non-x86_64 host is possible, then that will also work around this bug. This bug does not affect Wasmtime's or Cranelift's AArch64 backend, for example.</li>
</ul>
<h3 id="references">References</h3>
<ul>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size"><code>Config::static_memory_maximum_size</code></a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size"><code>Config::static_memory_guard_size</code></a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ">Mailing list announcement</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8">GitHub advisory</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f">Commit to fix this issue on Wasmtime's <code>main</code> branch</a></li>
</ul>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to us on <a href="https://bytecodealliance.zulipchat.com/#narrow/stream/217126-wasmtime">the Bytecode Alliance Zulip chat</a></li>
<li>Open an issue in <a href="https://github.com/bytecodealliance/wasmtime/">the bytecodealliance/wasmtime repository</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8">https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-26489">https://nvd.nist.gov/vuln/detail/CVE-2023-26489</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f">https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size">https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size">https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size</a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ">https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ</a></li>
<li><a href="https://github.com/advisories/GHSA-ff4p-7xrq-q5r8">https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</uri>
        </author>
        <category label="severity" term="CRITICAL"/>
        <published>2023-03-09T00:10:53.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[cranelift-codegen] wasmtime vulnerable to guest-controlled out-of-bounds read/write on x86_64]]></title>
        <id>https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</id>
        <link href="https://github.com/advisories/GHSA-ff4p-7xrq-q5r8"/>
        <updated>2023-03-09T00:10:54.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>Wasmtime's code generator, Cranelift, has a bug on x86_64 targets where address-mode computation mistakenly would calculate a 35-bit effective address instead of WebAssembly's defined 33-bit effective address. This bug means that, with default codegen settings, a wasm-controlled load/store operation could read/write addresses up to 35 bits away from the base of linear memory. Wasmtime's default sandbox settings provide up to 6G of protection from the base of linear memory to guarantee that any memory access in that range will be semantically correct. Due to this bug, however, addresses up to <code>0xffffffff * 8 + 0x7ffffffc = 36507222004 = ~34G</code> bytes away from the base of linear memory are possible from guest code. This means that the virtual memory 6G away from the base of linear memory up to ~34G away can be read/written by a malicious module.</p>
<p>This out of bounds read/write is not semantically correct and poses a threat as an arbitrary read/write within ~34G of linear memory away from the base of a wasm module's linear memory. A guest module can, without the knowledge of the embedder, read/write memory in this region. The memory may belong to other WebAssembly instances when using the pooling allocator, for example. The memory may also belong to the embedder, depending on address layout.</p>
<p>Embedders do not have a necessarily reliable means of detecting when this happens. Wasm loads/stores are allowed to cause machine segfaults meaning that an invalid read/write would be translated to a nominal WebAssembly trap. This means that a malicious module in the worst case silently reads/writes memory outside its bounds and in the "best" case looks like a normal "something trapped here" during its execution. This makes it difficult to retroactively determine whether this bug has been exploited on hosts. Affected embedders are recommended to analyze preexisting wasm modules to see if they're affected by the incorrect codegen rules and possibly correlate that with an anomalous number of traps during historical execution to locate possibly suspicious modules.</p>
<p>The specific bug in Cranelift's x86_64 backend is that a WebAssembly address which is left-shifted by a constant amount from 1 to 3 will get folded into x86_64's addressing modes which perform shifts. For example <code>(i32.load (i32.shl (local.get 0) (i32.const 3)))</code> loads from the WebAssembly address <code>$local0 &lt;&lt; 3</code>. When translated to Cranelift the <code>$local0 &lt;&lt; 3</code> computation, a 32-bit value, is zero-extended to a 64-bit value and then added to the base address of linear memory. Cranelift would generate an instruction of the form <code>movl (%base, %local0, 8), %dst</code> which calculates <code>%base + %local0 &lt;&lt; 3</code>. The bug here, however, is that the address computation happens with 64-bit values, where the <code>$local0 &lt;&lt; 3</code> computation was supposed to be truncated to a 32-bit value. This means that <code>%local0</code>, which can use up to 32-bits for an address, gets 3 extra bits of address space to be accessible via this <code>movl</code> instruction.</p>
<p>The fix in Cranelift is to remove the erroneous lowering rules in the backend which handle these zero-extended expressions. The above example is then translated to <code>movl %local0, %temp; shl $3, %temp; movl (%base, %temp), %dst</code> which correctly truncates the intermediate computation of <code>%local0 &lt;&lt; 3</code> to 32-bits inside the <code>%temp</code> register which is then added to the <code>%base</code> value.</p>
<h3 id="patches">Patches</h3>
<p>Wasmtime version 4.0.1, 5.0.1, and 6.0.1 have been released and have all been patched to no longer contain the erroneous lowering rules.</p>
<h3 id="workarounds">Workarounds</h3>
<p>While updating Wasmtime is recommended, there are a number of possible workarounds that embedders can employ to mitigate this issue if updating is not possible. Note that none of these workarounds are on-by-default and require explicit configuration:</p>
<ul>
<li>The <code>Config::static_memory_maximum_size(0)</code> option can be used to force all accesses to linear memory to be explicitly bounds-checked. This will perform a bounds check separately from the address-mode computation which correctly calculates the effective address of a load/store. Note that this can have a large impact on the execution performance of WebAssembly modules.</li>
<li>The <code>Config::static_memory_guard_size(1 &lt;&lt; 36)</code> option can be used to greatly increase the guard pages placed after linear memory. This will guarantee that memory accesses up-to-34G away are guaranteed to be semantically correct by reserving unmapped memory for the instance. Note that this reserves a very large amount of virtual memory per-instances and can greatly reduce the maximum number of concurrent instances being run.</li>
<li>If using a non-x86_64 host is possible, then that will also work around this bug. This bug does not affect Wasmtime's or Cranelift's AArch64 backend, for example.</li>
</ul>
<h3 id="references">References</h3>
<ul>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size"><code>Config::static_memory_maximum_size</code></a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size"><code>Config::static_memory_guard_size</code></a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ">Mailing list announcement</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8">GitHub advisory</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f">Commit to fix this issue on Wasmtime's <code>main</code> branch</a></li>
</ul>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to us on <a href="https://bytecodealliance.zulipchat.com/#narrow/stream/217126-wasmtime">the Bytecode Alliance Zulip chat</a></li>
<li>Open an issue in <a href="https://github.com/bytecodealliance/wasmtime/">the bytecodealliance/wasmtime repository</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8">https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-26489">https://nvd.nist.gov/vuln/detail/CVE-2023-26489</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f">https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size">https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size">https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size</a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ">https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ</a></li>
<li><a href="https://github.com/advisories/GHSA-ff4p-7xrq-q5r8">https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</uri>
        </author>
        <category label="severity" term="CRITICAL"/>
        <published>2023-03-09T00:10:53.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[cranelift-codegen] wasmtime vulnerable to guest-controlled out-of-bounds read/write on x86_64]]></title>
        <id>https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</id>
        <link href="https://github.com/advisories/GHSA-ff4p-7xrq-q5r8"/>
        <updated>2023-03-09T00:10:54.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>Wasmtime's code generator, Cranelift, has a bug on x86_64 targets where address-mode computation mistakenly would calculate a 35-bit effective address instead of WebAssembly's defined 33-bit effective address. This bug means that, with default codegen settings, a wasm-controlled load/store operation could read/write addresses up to 35 bits away from the base of linear memory. Wasmtime's default sandbox settings provide up to 6G of protection from the base of linear memory to guarantee that any memory access in that range will be semantically correct. Due to this bug, however, addresses up to <code>0xffffffff * 8 + 0x7ffffffc = 36507222004 = ~34G</code> bytes away from the base of linear memory are possible from guest code. This means that the virtual memory 6G away from the base of linear memory up to ~34G away can be read/written by a malicious module.</p>
<p>This out of bounds read/write is not semantically correct and poses a threat as an arbitrary read/write within ~34G of linear memory away from the base of a wasm module's linear memory. A guest module can, without the knowledge of the embedder, read/write memory in this region. The memory may belong to other WebAssembly instances when using the pooling allocator, for example. The memory may also belong to the embedder, depending on address layout.</p>
<p>Embedders do not have a necessarily reliable means of detecting when this happens. Wasm loads/stores are allowed to cause machine segfaults meaning that an invalid read/write would be translated to a nominal WebAssembly trap. This means that a malicious module in the worst case silently reads/writes memory outside its bounds and in the "best" case looks like a normal "something trapped here" during its execution. This makes it difficult to retroactively determine whether this bug has been exploited on hosts. Affected embedders are recommended to analyze preexisting wasm modules to see if they're affected by the incorrect codegen rules and possibly correlate that with an anomalous number of traps during historical execution to locate possibly suspicious modules.</p>
<p>The specific bug in Cranelift's x86_64 backend is that a WebAssembly address which is left-shifted by a constant amount from 1 to 3 will get folded into x86_64's addressing modes which perform shifts. For example <code>(i32.load (i32.shl (local.get 0) (i32.const 3)))</code> loads from the WebAssembly address <code>$local0 &lt;&lt; 3</code>. When translated to Cranelift the <code>$local0 &lt;&lt; 3</code> computation, a 32-bit value, is zero-extended to a 64-bit value and then added to the base address of linear memory. Cranelift would generate an instruction of the form <code>movl (%base, %local0, 8), %dst</code> which calculates <code>%base + %local0 &lt;&lt; 3</code>. The bug here, however, is that the address computation happens with 64-bit values, where the <code>$local0 &lt;&lt; 3</code> computation was supposed to be truncated to a 32-bit value. This means that <code>%local0</code>, which can use up to 32-bits for an address, gets 3 extra bits of address space to be accessible via this <code>movl</code> instruction.</p>
<p>The fix in Cranelift is to remove the erroneous lowering rules in the backend which handle these zero-extended expressions. The above example is then translated to <code>movl %local0, %temp; shl $3, %temp; movl (%base, %temp), %dst</code> which correctly truncates the intermediate computation of <code>%local0 &lt;&lt; 3</code> to 32-bits inside the <code>%temp</code> register which is then added to the <code>%base</code> value.</p>
<h3 id="patches">Patches</h3>
<p>Wasmtime version 4.0.1, 5.0.1, and 6.0.1 have been released and have all been patched to no longer contain the erroneous lowering rules.</p>
<h3 id="workarounds">Workarounds</h3>
<p>While updating Wasmtime is recommended, there are a number of possible workarounds that embedders can employ to mitigate this issue if updating is not possible. Note that none of these workarounds are on-by-default and require explicit configuration:</p>
<ul>
<li>The <code>Config::static_memory_maximum_size(0)</code> option can be used to force all accesses to linear memory to be explicitly bounds-checked. This will perform a bounds check separately from the address-mode computation which correctly calculates the effective address of a load/store. Note that this can have a large impact on the execution performance of WebAssembly modules.</li>
<li>The <code>Config::static_memory_guard_size(1 &lt;&lt; 36)</code> option can be used to greatly increase the guard pages placed after linear memory. This will guarantee that memory accesses up-to-34G away are guaranteed to be semantically correct by reserving unmapped memory for the instance. Note that this reserves a very large amount of virtual memory per-instances and can greatly reduce the maximum number of concurrent instances being run.</li>
<li>If using a non-x86_64 host is possible, then that will also work around this bug. This bug does not affect Wasmtime's or Cranelift's AArch64 backend, for example.</li>
</ul>
<h3 id="references">References</h3>
<ul>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size"><code>Config::static_memory_maximum_size</code></a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size"><code>Config::static_memory_guard_size</code></a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ">Mailing list announcement</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8">GitHub advisory</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f">Commit to fix this issue on Wasmtime's <code>main</code> branch</a></li>
</ul>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to us on <a href="https://bytecodealliance.zulipchat.com/#narrow/stream/217126-wasmtime">the Bytecode Alliance Zulip chat</a></li>
<li>Open an issue in <a href="https://github.com/bytecodealliance/wasmtime/">the bytecodealliance/wasmtime repository</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8">https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-26489">https://nvd.nist.gov/vuln/detail/CVE-2023-26489</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f">https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size">https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size">https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size</a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ">https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ</a></li>
<li><a href="https://github.com/advisories/GHSA-ff4p-7xrq-q5r8">https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</uri>
        </author>
        <category label="severity" term="CRITICAL"/>
        <published>2023-03-09T00:10:53.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[wasmtime] wasmtime vulnerable to guest-controlled out-of-bounds read/write on x86_64]]></title>
        <id>https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</id>
        <link href="https://github.com/advisories/GHSA-ff4p-7xrq-q5r8"/>
        <updated>2023-03-09T00:10:54.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>Wasmtime's code generator, Cranelift, has a bug on x86_64 targets where address-mode computation mistakenly would calculate a 35-bit effective address instead of WebAssembly's defined 33-bit effective address. This bug means that, with default codegen settings, a wasm-controlled load/store operation could read/write addresses up to 35 bits away from the base of linear memory. Wasmtime's default sandbox settings provide up to 6G of protection from the base of linear memory to guarantee that any memory access in that range will be semantically correct. Due to this bug, however, addresses up to <code>0xffffffff * 8 + 0x7ffffffc = 36507222004 = ~34G</code> bytes away from the base of linear memory are possible from guest code. This means that the virtual memory 6G away from the base of linear memory up to ~34G away can be read/written by a malicious module.</p>
<p>This out of bounds read/write is not semantically correct and poses a threat as an arbitrary read/write within ~34G of linear memory away from the base of a wasm module's linear memory. A guest module can, without the knowledge of the embedder, read/write memory in this region. The memory may belong to other WebAssembly instances when using the pooling allocator, for example. The memory may also belong to the embedder, depending on address layout.</p>
<p>Embedders do not have a necessarily reliable means of detecting when this happens. Wasm loads/stores are allowed to cause machine segfaults meaning that an invalid read/write would be translated to a nominal WebAssembly trap. This means that a malicious module in the worst case silently reads/writes memory outside its bounds and in the "best" case looks like a normal "something trapped here" during its execution. This makes it difficult to retroactively determine whether this bug has been exploited on hosts. Affected embedders are recommended to analyze preexisting wasm modules to see if they're affected by the incorrect codegen rules and possibly correlate that with an anomalous number of traps during historical execution to locate possibly suspicious modules.</p>
<p>The specific bug in Cranelift's x86_64 backend is that a WebAssembly address which is left-shifted by a constant amount from 1 to 3 will get folded into x86_64's addressing modes which perform shifts. For example <code>(i32.load (i32.shl (local.get 0) (i32.const 3)))</code> loads from the WebAssembly address <code>$local0 &lt;&lt; 3</code>. When translated to Cranelift the <code>$local0 &lt;&lt; 3</code> computation, a 32-bit value, is zero-extended to a 64-bit value and then added to the base address of linear memory. Cranelift would generate an instruction of the form <code>movl (%base, %local0, 8), %dst</code> which calculates <code>%base + %local0 &lt;&lt; 3</code>. The bug here, however, is that the address computation happens with 64-bit values, where the <code>$local0 &lt;&lt; 3</code> computation was supposed to be truncated to a 32-bit value. This means that <code>%local0</code>, which can use up to 32-bits for an address, gets 3 extra bits of address space to be accessible via this <code>movl</code> instruction.</p>
<p>The fix in Cranelift is to remove the erroneous lowering rules in the backend which handle these zero-extended expressions. The above example is then translated to <code>movl %local0, %temp; shl $3, %temp; movl (%base, %temp), %dst</code> which correctly truncates the intermediate computation of <code>%local0 &lt;&lt; 3</code> to 32-bits inside the <code>%temp</code> register which is then added to the <code>%base</code> value.</p>
<h3 id="patches">Patches</h3>
<p>Wasmtime version 4.0.1, 5.0.1, and 6.0.1 have been released and have all been patched to no longer contain the erroneous lowering rules.</p>
<h3 id="workarounds">Workarounds</h3>
<p>While updating Wasmtime is recommended, there are a number of possible workarounds that embedders can employ to mitigate this issue if updating is not possible. Note that none of these workarounds are on-by-default and require explicit configuration:</p>
<ul>
<li>The <code>Config::static_memory_maximum_size(0)</code> option can be used to force all accesses to linear memory to be explicitly bounds-checked. This will perform a bounds check separately from the address-mode computation which correctly calculates the effective address of a load/store. Note that this can have a large impact on the execution performance of WebAssembly modules.</li>
<li>The <code>Config::static_memory_guard_size(1 &lt;&lt; 36)</code> option can be used to greatly increase the guard pages placed after linear memory. This will guarantee that memory accesses up-to-34G away are guaranteed to be semantically correct by reserving unmapped memory for the instance. Note that this reserves a very large amount of virtual memory per-instances and can greatly reduce the maximum number of concurrent instances being run.</li>
<li>If using a non-x86_64 host is possible, then that will also work around this bug. This bug does not affect Wasmtime's or Cranelift's AArch64 backend, for example.</li>
</ul>
<h3 id="references">References</h3>
<ul>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size"><code>Config::static_memory_maximum_size</code></a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size"><code>Config::static_memory_guard_size</code></a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ">Mailing list announcement</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8">GitHub advisory</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f">Commit to fix this issue on Wasmtime's <code>main</code> branch</a></li>
</ul>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to us on <a href="https://bytecodealliance.zulipchat.com/#narrow/stream/217126-wasmtime">the Bytecode Alliance Zulip chat</a></li>
<li>Open an issue in <a href="https://github.com/bytecodealliance/wasmtime/">the bytecodealliance/wasmtime repository</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8">https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-26489">https://nvd.nist.gov/vuln/detail/CVE-2023-26489</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f">https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size">https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size">https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size</a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ">https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ</a></li>
<li><a href="https://github.com/advisories/GHSA-ff4p-7xrq-q5r8">https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</uri>
        </author>
        <category label="severity" term="CRITICAL"/>
        <published>2023-03-09T00:10:53.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[wasmtime] wasmtime vulnerable to guest-controlled out-of-bounds read/write on x86_64]]></title>
        <id>https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</id>
        <link href="https://github.com/advisories/GHSA-ff4p-7xrq-q5r8"/>
        <updated>2023-03-09T00:10:54.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>Wasmtime's code generator, Cranelift, has a bug on x86_64 targets where address-mode computation mistakenly would calculate a 35-bit effective address instead of WebAssembly's defined 33-bit effective address. This bug means that, with default codegen settings, a wasm-controlled load/store operation could read/write addresses up to 35 bits away from the base of linear memory. Wasmtime's default sandbox settings provide up to 6G of protection from the base of linear memory to guarantee that any memory access in that range will be semantically correct. Due to this bug, however, addresses up to <code>0xffffffff * 8 + 0x7ffffffc = 36507222004 = ~34G</code> bytes away from the base of linear memory are possible from guest code. This means that the virtual memory 6G away from the base of linear memory up to ~34G away can be read/written by a malicious module.</p>
<p>This out of bounds read/write is not semantically correct and poses a threat as an arbitrary read/write within ~34G of linear memory away from the base of a wasm module's linear memory. A guest module can, without the knowledge of the embedder, read/write memory in this region. The memory may belong to other WebAssembly instances when using the pooling allocator, for example. The memory may also belong to the embedder, depending on address layout.</p>
<p>Embedders do not have a necessarily reliable means of detecting when this happens. Wasm loads/stores are allowed to cause machine segfaults meaning that an invalid read/write would be translated to a nominal WebAssembly trap. This means that a malicious module in the worst case silently reads/writes memory outside its bounds and in the "best" case looks like a normal "something trapped here" during its execution. This makes it difficult to retroactively determine whether this bug has been exploited on hosts. Affected embedders are recommended to analyze preexisting wasm modules to see if they're affected by the incorrect codegen rules and possibly correlate that with an anomalous number of traps during historical execution to locate possibly suspicious modules.</p>
<p>The specific bug in Cranelift's x86_64 backend is that a WebAssembly address which is left-shifted by a constant amount from 1 to 3 will get folded into x86_64's addressing modes which perform shifts. For example <code>(i32.load (i32.shl (local.get 0) (i32.const 3)))</code> loads from the WebAssembly address <code>$local0 &lt;&lt; 3</code>. When translated to Cranelift the <code>$local0 &lt;&lt; 3</code> computation, a 32-bit value, is zero-extended to a 64-bit value and then added to the base address of linear memory. Cranelift would generate an instruction of the form <code>movl (%base, %local0, 8), %dst</code> which calculates <code>%base + %local0 &lt;&lt; 3</code>. The bug here, however, is that the address computation happens with 64-bit values, where the <code>$local0 &lt;&lt; 3</code> computation was supposed to be truncated to a 32-bit value. This means that <code>%local0</code>, which can use up to 32-bits for an address, gets 3 extra bits of address space to be accessible via this <code>movl</code> instruction.</p>
<p>The fix in Cranelift is to remove the erroneous lowering rules in the backend which handle these zero-extended expressions. The above example is then translated to <code>movl %local0, %temp; shl $3, %temp; movl (%base, %temp), %dst</code> which correctly truncates the intermediate computation of <code>%local0 &lt;&lt; 3</code> to 32-bits inside the <code>%temp</code> register which is then added to the <code>%base</code> value.</p>
<h3 id="patches">Patches</h3>
<p>Wasmtime version 4.0.1, 5.0.1, and 6.0.1 have been released and have all been patched to no longer contain the erroneous lowering rules.</p>
<h3 id="workarounds">Workarounds</h3>
<p>While updating Wasmtime is recommended, there are a number of possible workarounds that embedders can employ to mitigate this issue if updating is not possible. Note that none of these workarounds are on-by-default and require explicit configuration:</p>
<ul>
<li>The <code>Config::static_memory_maximum_size(0)</code> option can be used to force all accesses to linear memory to be explicitly bounds-checked. This will perform a bounds check separately from the address-mode computation which correctly calculates the effective address of a load/store. Note that this can have a large impact on the execution performance of WebAssembly modules.</li>
<li>The <code>Config::static_memory_guard_size(1 &lt;&lt; 36)</code> option can be used to greatly increase the guard pages placed after linear memory. This will guarantee that memory accesses up-to-34G away are guaranteed to be semantically correct by reserving unmapped memory for the instance. Note that this reserves a very large amount of virtual memory per-instances and can greatly reduce the maximum number of concurrent instances being run.</li>
<li>If using a non-x86_64 host is possible, then that will also work around this bug. This bug does not affect Wasmtime's or Cranelift's AArch64 backend, for example.</li>
</ul>
<h3 id="references">References</h3>
<ul>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size"><code>Config::static_memory_maximum_size</code></a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size"><code>Config::static_memory_guard_size</code></a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ">Mailing list announcement</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8">GitHub advisory</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f">Commit to fix this issue on Wasmtime's <code>main</code> branch</a></li>
</ul>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to us on <a href="https://bytecodealliance.zulipchat.com/#narrow/stream/217126-wasmtime">the Bytecode Alliance Zulip chat</a></li>
<li>Open an issue in <a href="https://github.com/bytecodealliance/wasmtime/">the bytecodealliance/wasmtime repository</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8">https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-26489">https://nvd.nist.gov/vuln/detail/CVE-2023-26489</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f">https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size">https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size">https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size</a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ">https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ</a></li>
<li><a href="https://github.com/advisories/GHSA-ff4p-7xrq-q5r8">https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</uri>
        </author>
        <category label="severity" term="CRITICAL"/>
        <published>2023-03-09T00:10:53.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[wasmtime] wasmtime vulnerable to guest-controlled out-of-bounds read/write on x86_64]]></title>
        <id>https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</id>
        <link href="https://github.com/advisories/GHSA-ff4p-7xrq-q5r8"/>
        <updated>2023-03-09T00:10:54.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>Wasmtime's code generator, Cranelift, has a bug on x86_64 targets where address-mode computation mistakenly would calculate a 35-bit effective address instead of WebAssembly's defined 33-bit effective address. This bug means that, with default codegen settings, a wasm-controlled load/store operation could read/write addresses up to 35 bits away from the base of linear memory. Wasmtime's default sandbox settings provide up to 6G of protection from the base of linear memory to guarantee that any memory access in that range will be semantically correct. Due to this bug, however, addresses up to <code>0xffffffff * 8 + 0x7ffffffc = 36507222004 = ~34G</code> bytes away from the base of linear memory are possible from guest code. This means that the virtual memory 6G away from the base of linear memory up to ~34G away can be read/written by a malicious module.</p>
<p>This out of bounds read/write is not semantically correct and poses a threat as an arbitrary read/write within ~34G of linear memory away from the base of a wasm module's linear memory. A guest module can, without the knowledge of the embedder, read/write memory in this region. The memory may belong to other WebAssembly instances when using the pooling allocator, for example. The memory may also belong to the embedder, depending on address layout.</p>
<p>Embedders do not have a necessarily reliable means of detecting when this happens. Wasm loads/stores are allowed to cause machine segfaults meaning that an invalid read/write would be translated to a nominal WebAssembly trap. This means that a malicious module in the worst case silently reads/writes memory outside its bounds and in the "best" case looks like a normal "something trapped here" during its execution. This makes it difficult to retroactively determine whether this bug has been exploited on hosts. Affected embedders are recommended to analyze preexisting wasm modules to see if they're affected by the incorrect codegen rules and possibly correlate that with an anomalous number of traps during historical execution to locate possibly suspicious modules.</p>
<p>The specific bug in Cranelift's x86_64 backend is that a WebAssembly address which is left-shifted by a constant amount from 1 to 3 will get folded into x86_64's addressing modes which perform shifts. For example <code>(i32.load (i32.shl (local.get 0) (i32.const 3)))</code> loads from the WebAssembly address <code>$local0 &lt;&lt; 3</code>. When translated to Cranelift the <code>$local0 &lt;&lt; 3</code> computation, a 32-bit value, is zero-extended to a 64-bit value and then added to the base address of linear memory. Cranelift would generate an instruction of the form <code>movl (%base, %local0, 8), %dst</code> which calculates <code>%base + %local0 &lt;&lt; 3</code>. The bug here, however, is that the address computation happens with 64-bit values, where the <code>$local0 &lt;&lt; 3</code> computation was supposed to be truncated to a 32-bit value. This means that <code>%local0</code>, which can use up to 32-bits for an address, gets 3 extra bits of address space to be accessible via this <code>movl</code> instruction.</p>
<p>The fix in Cranelift is to remove the erroneous lowering rules in the backend which handle these zero-extended expressions. The above example is then translated to <code>movl %local0, %temp; shl $3, %temp; movl (%base, %temp), %dst</code> which correctly truncates the intermediate computation of <code>%local0 &lt;&lt; 3</code> to 32-bits inside the <code>%temp</code> register which is then added to the <code>%base</code> value.</p>
<h3 id="patches">Patches</h3>
<p>Wasmtime version 4.0.1, 5.0.1, and 6.0.1 have been released and have all been patched to no longer contain the erroneous lowering rules.</p>
<h3 id="workarounds">Workarounds</h3>
<p>While updating Wasmtime is recommended, there are a number of possible workarounds that embedders can employ to mitigate this issue if updating is not possible. Note that none of these workarounds are on-by-default and require explicit configuration:</p>
<ul>
<li>The <code>Config::static_memory_maximum_size(0)</code> option can be used to force all accesses to linear memory to be explicitly bounds-checked. This will perform a bounds check separately from the address-mode computation which correctly calculates the effective address of a load/store. Note that this can have a large impact on the execution performance of WebAssembly modules.</li>
<li>The <code>Config::static_memory_guard_size(1 &lt;&lt; 36)</code> option can be used to greatly increase the guard pages placed after linear memory. This will guarantee that memory accesses up-to-34G away are guaranteed to be semantically correct by reserving unmapped memory for the instance. Note that this reserves a very large amount of virtual memory per-instances and can greatly reduce the maximum number of concurrent instances being run.</li>
<li>If using a non-x86_64 host is possible, then that will also work around this bug. This bug does not affect Wasmtime's or Cranelift's AArch64 backend, for example.</li>
</ul>
<h3 id="references">References</h3>
<ul>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size"><code>Config::static_memory_maximum_size</code></a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size"><code>Config::static_memory_guard_size</code></a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ">Mailing list announcement</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8">GitHub advisory</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f">Commit to fix this issue on Wasmtime's <code>main</code> branch</a></li>
</ul>
<h3 id="for-more-information">For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to us on <a href="https://bytecodealliance.zulipchat.com/#narrow/stream/217126-wasmtime">the Bytecode Alliance Zulip chat</a></li>
<li>Open an issue in <a href="https://github.com/bytecodealliance/wasmtime/">the bytecodealliance/wasmtime repository</a></li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8">https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-ff4p-7xrq-q5r8</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2023-26489">https://nvd.nist.gov/vuln/detail/CVE-2023-26489</a></li>
<li><a href="https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f">https://github.com/bytecodealliance/wasmtime/commit/63fb30e4b4415455d47b3da5a19d79c12f4f2d1f</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size">https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_guard_size</a></li>
<li><a href="https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size">https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.static_memory_maximum_size</a></li>
<li><a href="https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ">https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ</a></li>
<li><a href="https://github.com/advisories/GHSA-ff4p-7xrq-q5r8">https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-ff4p-7xrq-q5r8</uri>
        </author>
        <category label="severity" term="CRITICAL"/>
        <published>2023-03-09T00:10:53.000Z</published>
    </entry>
</feed>